nohup: ignoring input
25-11-18 02:01:53.611 - INFO:   name: editguard_coco_finetune
  model: MIMO-VRN-h
  scale: 4
  gpu_ids: [0]
  gop: 1
  num_image: 1
  nf: 12
  is_train: True
  distortion: none
  addnoise: False
  addjpeg: False
  addpossion: False
  sdinpaint: False
  controlnetinpaint: False
  sdxl: False
  repaint: False
  hide: True
  bithide: False
  degrade_shuffle: False
  prompt: True
  prompt_len: 3
  message_length: 64
  losstype: mse
  mode: image
  use_tb_logger: True
  dist: False
  datasets:[
    train:[
      name: COCO2017_train
      mode: train
      dataroot_GT: /mnt/ssd/BG/EditGuard/EditGuard/COCO2017/train2017/train2017
      txt_path: /mnt/ssd/BG/EditGuard/EditGuard/COCO2017/train2017.txt
      resume_state: /mnt/ssd/BG/EditGuard/EditGuard/experiments/editguard_coco_finetune/training_state/200000.state
      pretrain_model_G: None
      interval_list: [1]
      random_reverse: False
      border_mode: False
      N_frames: 1
      use_shuffle: True
      n_workers: 8
      batch_size: 1
      GT_size: 256
      color: RGB
      use_flip: True
      use_rot: True
      phase: train
      data_type: img
    ]
    val:[
      name: COCO2017_val
      mode: train
      dataroot_GT: /mnt/ssd/BG/EditGuard/EditGuard/COCO2017/train2017/train2017
      txt_path: /mnt/ssd/BG/EditGuard/EditGuard/COCO2017/train2017.txt
      interval_list: [1]
      random_reverse: False
      border_mode: False
      N_frames: 1
      GT_size: 256
      phase: val
      data_type: img
    ]
  ]
  network_G:[
    which_model_G: IBSN
    subnet_type: DBNet
    in_nc: 12
    out_nc: 12
    block_num: [6, 6]
    scale: 2
    init: xavier_group
    block_num_rbm: 8
    block_num_trans: 4
  ]
  path:[
    pretrain_model_G: /mnt/ssd/BG/EditGuard/EditGuard/checkpoints/clean.pth
    strict_load: False
    resume_state: None
    root: /mnt/ssd/BG/EditGuard/EditGuard
    experiments_root: /mnt/ssd/BG/EditGuard/EditGuard/experiments/editguard_coco_finetune
    models: /mnt/ssd/BG/EditGuard/EditGuard/experiments/editguard_coco_finetune/models
    training_state: /mnt/ssd/BG/EditGuard/EditGuard/experiments/editguard_coco_finetune/training_state
    log: /mnt/ssd/BG/EditGuard/EditGuard/experiments/editguard_coco_finetune
    val_images: /mnt/ssd/BG/EditGuard/EditGuard/experiments/editguard_coco_finetune/val_images
  ]
  train:[
    lr_G: 2e-4
    beta1: 0.9
    beta2: 0.999
    val_freq: 5000
    save_freq: 5000
    niter: 350000
    lr_scheme: MultiStepLR
    lr_steps: [50000, 100000, 150000]
    lr_gamma: 0.5
    weight_decay_G: 0
    N_frames: 1
    warmup_iter: 0
    pixel_criterion_forw: l2
    lambda_fit_forw: 1.0
    lambda_rec_back: 1.0
    lambda_crossover: 1.0
    lambda_bit: 1.0
  ]
  loss:[
    pixel_weight: 1.0
    vgg_weight: 1.0
    lpips_weight: 1.0
  ]
  logger:[
    display_freq: 100
    save_checkpoint_freq: 2500
    print_freq: 100
  ]
  use_amp: True

25-11-18 02:01:53.659 - INFO: Random seed: 2666
25-11-18 02:01:53.682 - INFO: Temporal augmentation interval list: [1], with random reverse is False.
25-11-18 02:01:53.727 - INFO: Dataset [CoCoDataset - COCO2017_train] is created.
25-11-18 02:01:53.727 - INFO: Number of train images: 118,287, iters: 118,287
25-11-18 02:01:53.728 - INFO: Total epochs needed: 3 for iters 350,000
25-11-18 02:01:53.746 - INFO: Temporal augmentation interval list: [1], with random reverse is False.
25-11-18 02:01:53.779 - INFO: Dataset [CoCoDataset - COCO2017_val] is created.
25-11-18 02:01:53.779 - INFO: Number of val images in [COCO2017_val]: 118287
25-11-18 02:01:54.945 - INFO: Network G structure: DataParallel - VSN, with parameters: 7,782,651
25-11-18 02:01:54.946 - INFO: VSN(
  (bitencoder): DW_Encoder(
    (conv1): ConvBlock(
      (layers): Sequential(
        (0): ConvINRelu(
          (layers): Sequential(
            (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
          )
        )
        (1): ConvINRelu(
          (layers): Sequential(
            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
          )
        )
      )
    )
    (down1): Down(
      (layer): Sequential(
        (0): ConvBlock(
          (layers): Sequential(
            (0): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
          )
        )
        (1): ConvBlock(
          (layers): Sequential(
            (0): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
            (1): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
          )
        )
      )
    )
    (down2): Down(
      (layer): Sequential(
        (0): ConvBlock(
          (layers): Sequential(
            (0): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
          )
        )
        (1): ConvBlock(
          (layers): Sequential(
            (0): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
            (1): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
          )
        )
      )
    )
    (down3): Down(
      (layer): Sequential(
        (0): ConvBlock(
          (layers): Sequential(
            (0): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
          )
        )
        (1): ConvBlock(
          (layers): Sequential(
            (0): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
            (1): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
          )
        )
      )
    )
    (down4): Down(
      (layer): Sequential(
        (0): ConvBlock(
          (layers): Sequential(
            (0): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
          )
        )
        (1): ConvBlock(
          (layers): Sequential(
            (0): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
            (1): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
          )
        )
      )
    )
    (up3): UP(
      (conv): ConvBlock(
        (layers): Sequential(
          (0): ConvINRelu(
            (layers): Sequential(
              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (2): ReLU(inplace=True)
            )
          )
        )
      )
    )
    (linear3): Linear(in_features=64, out_features=4096, bias=True)
    (Conv_message3): ConvBlock(
      (layers): Sequential(
        (0): ConvINRelu(
          (layers): Sequential(
            (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
          )
        )
        (1): ConvINRelu(
          (layers): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
          )
        )
      )
    )
    (att3): ResBlock(
      (layers): Sequential(
        (0): BottleneckBlock(
          (change): Sequential(
            (0): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (left): Sequential(
            (0): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (5): ReLU(inplace=True)
            (6): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (attention): SEAttention(
            (se): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (2): ReLU(inplace=True)
              (3): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): Sigmoid()
            )
          )
        )
        (1): BottleneckBlock(
          (left): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (5): ReLU(inplace=True)
            (6): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (attention): SEAttention(
            (se): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (2): ReLU(inplace=True)
              (3): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): Sigmoid()
            )
          )
        )
      )
    )
    (up2): UP(
      (conv): ConvBlock(
        (layers): Sequential(
          (0): ConvINRelu(
            (layers): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (2): ReLU(inplace=True)
            )
          )
        )
      )
    )
    (linear2): Linear(in_features=64, out_features=4096, bias=True)
    (Conv_message2): ConvBlock(
      (layers): Sequential(
        (0): ConvINRelu(
          (layers): Sequential(
            (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
          )
        )
        (1): ConvINRelu(
          (layers): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
          )
        )
      )
    )
    (att2): ResBlock(
      (layers): Sequential(
        (0): BottleneckBlock(
          (change): Sequential(
            (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (left): Sequential(
            (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (5): ReLU(inplace=True)
            (6): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (attention): SEAttention(
            (se): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (2): ReLU(inplace=True)
              (3): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): Sigmoid()
            )
          )
        )
        (1): BottleneckBlock(
          (left): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (5): ReLU(inplace=True)
            (6): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (attention): SEAttention(
            (se): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (2): ReLU(inplace=True)
              (3): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): Sigmoid()
            )
          )
        )
      )
    )
    (up1): UP(
      (conv): ConvBlock(
        (layers): Sequential(
          (0): ConvINRelu(
            (layers): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (2): ReLU(inplace=True)
            )
          )
        )
      )
    )
    (linear1): Linear(in_features=64, out_features=4096, bias=True)
    (Conv_message1): ConvBlock(
      (layers): Sequential(
        (0): ConvINRelu(
          (layers): Sequential(
            (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
          )
        )
        (1): ConvINRelu(
          (layers): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
          )
        )
      )
    )
    (att1): ResBlock(
      (layers): Sequential(
        (0): BottleneckBlock(
          (change): Sequential(
            (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (left): Sequential(
            (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (5): ReLU(inplace=True)
            (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (attention): SEAttention(
            (se): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (2): ReLU(inplace=True)
              (3): Conv2d(4, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): Sigmoid()
            )
          )
        )
        (1): BottleneckBlock(
          (left): Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (5): ReLU(inplace=True)
            (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (attention): SEAttention(
            (se): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (2): ReLU(inplace=True)
              (3): Conv2d(4, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): Sigmoid()
            )
          )
        )
      )
    )
    (up0): UP(
      (conv): ConvBlock(
        (layers): Sequential(
          (0): ConvINRelu(
            (layers): Sequential(
              (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (2): ReLU(inplace=True)
            )
          )
        )
      )
    )
    (linear0): Linear(in_features=64, out_features=4096, bias=True)
    (Conv_message0): ConvBlock(
      (layers): Sequential(
        (0): ConvINRelu(
          (layers): Sequential(
            (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
          )
        )
        (1): ConvINRelu(
          (layers): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
          )
        )
      )
    )
    (att0): ResBlock(
      (layers): Sequential(
        (0): BottleneckBlock(
          (change): Sequential(
            (0): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (left): Sequential(
            (0): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (5): ReLU(inplace=True)
            (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (attention): SEAttention(
            (se): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(16, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (2): ReLU(inplace=True)
              (3): Conv2d(2, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): Sigmoid()
            )
          )
        )
        (1): BottleneckBlock(
          (left): Sequential(
            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (5): ReLU(inplace=True)
            (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (attention): SEAttention(
            (se): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(16, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (2): ReLU(inplace=True)
              (3): Conv2d(2, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): Sigmoid()
            )
          )
        )
      )
    )
    (Conv_1x1): Conv2d(19, 3, kernel_size=(1, 1), stride=(1, 1))
  )
  (bitdecoder): DW_Decoder(
    (conv1): ConvBlock(
      (layers): Sequential(
        (0): ConvINRelu(
          (layers): Sequential(
            (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
          )
        )
        (1): ConvINRelu(
          (layers): Sequential(
            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
          )
        )
      )
    )
    (down1): Down(
      (layer): Sequential(
        (0): ConvBlock(
          (layers): Sequential(
            (0): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
          )
        )
        (1): ConvBlock(
          (layers): Sequential(
            (0): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
            (1): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
          )
        )
      )
    )
    (down2): Down(
      (layer): Sequential(
        (0): ConvBlock(
          (layers): Sequential(
            (0): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
          )
        )
        (1): ConvBlock(
          (layers): Sequential(
            (0): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
            (1): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
          )
        )
      )
    )
    (down3): Down(
      (layer): Sequential(
        (0): ConvBlock(
          (layers): Sequential(
            (0): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
          )
        )
        (1): ConvBlock(
          (layers): Sequential(
            (0): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
            (1): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
          )
        )
      )
    )
    (down4): Down(
      (layer): Sequential(
        (0): ConvBlock(
          (layers): Sequential(
            (0): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
          )
        )
        (1): ConvBlock(
          (layers): Sequential(
            (0): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
            (1): ConvINRelu(
              (layers): Sequential(
                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
                (2): ReLU(inplace=True)
              )
            )
          )
        )
      )
    )
    (up3): UP(
      (conv): ConvBlock(
        (layers): Sequential(
          (0): ConvINRelu(
            (layers): Sequential(
              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (2): ReLU(inplace=True)
            )
          )
        )
      )
    )
    (att3): ResBlock(
      (layers): Sequential(
        (0): BottleneckBlock(
          (change): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (left): Sequential(
            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (5): ReLU(inplace=True)
            (6): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (attention): SEAttention(
            (se): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (2): ReLU(inplace=True)
              (3): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): Sigmoid()
            )
          )
        )
        (1): BottleneckBlock(
          (left): Sequential(
            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (5): ReLU(inplace=True)
            (6): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (attention): SEAttention(
            (se): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (2): ReLU(inplace=True)
              (3): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): Sigmoid()
            )
          )
        )
      )
    )
    (up2): UP(
      (conv): ConvBlock(
        (layers): Sequential(
          (0): ConvINRelu(
            (layers): Sequential(
              (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (2): ReLU(inplace=True)
            )
          )
        )
      )
    )
    (att2): ResBlock(
      (layers): Sequential(
        (0): BottleneckBlock(
          (change): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (left): Sequential(
            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (5): ReLU(inplace=True)
            (6): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (attention): SEAttention(
            (se): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (2): ReLU(inplace=True)
              (3): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): Sigmoid()
            )
          )
        )
        (1): BottleneckBlock(
          (left): Sequential(
            (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (5): ReLU(inplace=True)
            (6): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (attention): SEAttention(
            (se): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (2): ReLU(inplace=True)
              (3): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): Sigmoid()
            )
          )
        )
      )
    )
    (up1): UP(
      (conv): ConvBlock(
        (layers): Sequential(
          (0): ConvINRelu(
            (layers): Sequential(
              (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (2): ReLU(inplace=True)
            )
          )
        )
      )
    )
    (att1): ResBlock(
      (layers): Sequential(
        (0): BottleneckBlock(
          (change): Sequential(
            (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (left): Sequential(
            (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (5): ReLU(inplace=True)
            (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (attention): SEAttention(
            (se): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (2): ReLU(inplace=True)
              (3): Conv2d(4, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): Sigmoid()
            )
          )
        )
        (1): BottleneckBlock(
          (left): Sequential(
            (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (5): ReLU(inplace=True)
            (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (attention): SEAttention(
            (se): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (2): ReLU(inplace=True)
              (3): Conv2d(4, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): Sigmoid()
            )
          )
        )
      )
    )
    (up0): UP(
      (conv): ConvBlock(
        (layers): Sequential(
          (0): ConvINRelu(
            (layers): Sequential(
              (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
              (2): ReLU(inplace=True)
            )
          )
        )
      )
    )
    (att0): ResBlock(
      (layers): Sequential(
        (0): BottleneckBlock(
          (change): Sequential(
            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (left): Sequential(
            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (5): ReLU(inplace=True)
            (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (attention): SEAttention(
            (se): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(16, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (2): ReLU(inplace=True)
              (3): Conv2d(2, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): Sigmoid()
            )
          )
        )
        (1): BottleneckBlock(
          (left): Sequential(
            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (5): ReLU(inplace=True)
            (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (7): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
          (attention): SEAttention(
            (se): Sequential(
              (0): AdaptiveAvgPool2d(output_size=(1, 1))
              (1): Conv2d(16, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (2): ReLU(inplace=True)
              (3): Conv2d(2, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (4): Sigmoid()
            )
          )
        )
      )
    )
    (Conv_1x1): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (message_layer): Linear(in_features=4096, out_features=64, bias=True)
  )
  (irn): InvNN(
    (operations): ModuleList(
      (0-5): 6 x InvBlock(
        (F): DenseBlock(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)
        )
        (NF): NAFBlock(
          (conv1): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1))
          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
          (conv3): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))
          (sca): Sequential(
            (0): AdaptiveAvgPool2d(output_size=1)
            (1): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))
          )
          (sg): SimpleGate()
          (conv4): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1))
          (conv5): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))
          (norm1): LayerNorm2d()
          (norm2): LayerNorm2d()
          (dropout1): Identity()
          (dropout2): Identity()
        )
        (G): DenseBlock(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)
        )
        (NG): NAFBlock(
          (conv1): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1))
          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
          (conv3): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))
          (sca): Sequential(
            (0): AdaptiveAvgPool2d(output_size=1)
            (1): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))
          )
          (sg): SimpleGate()
          (conv4): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1))
          (conv5): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))
          (norm1): LayerNorm2d()
          (norm2): LayerNorm2d()
          (dropout1): Identity()
          (dropout2): Identity()
        )
        (H): DenseBlock(
          (conv1): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv2): Conv2d(44, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv3): Conv2d(76, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv4): Conv2d(108, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (conv5): Conv2d(140, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)
        )
        (NH): NAFBlock(
          (conv1): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1))
          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
          (conv3): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))
          (sca): Sequential(
            (0): AdaptiveAvgPool2d(output_size=1)
            (1): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))
          )
          (sg): SimpleGate()
          (conv4): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1))
          (conv5): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1))
          (norm1): LayerNorm2d()
          (norm2): LayerNorm2d()
          (dropout1): Identity()
          (dropout2): Identity()
        )
      )
    )
  )
  (pm): PredictiveModuleMIMO_prompt(
    (conv_in): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (res_block): Sequential(
      (0): ResidualBlockNoBN(
        (conv1): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (relu): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (1): ResidualBlockNoBN(
        (conv1): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (relu): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (2): ResidualBlockNoBN(
        (conv1): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (relu): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (3): ResidualBlockNoBN(
        (conv1): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (relu): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (4): ResidualBlockNoBN(
        (conv1): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (relu): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (5): ResidualBlockNoBN(
        (conv1): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (relu): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (6): ResidualBlockNoBN(
        (conv1): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (relu): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (7): ResidualBlockNoBN(
        (conv1): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (relu): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (transformer_block): Sequential(
      (0): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(12, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)
          (project_out): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(12, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (project_out): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (1): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(12, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)
          (project_out): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(12, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (project_out): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (2): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(12, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)
          (project_out): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(12, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (project_out): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
      (3): TransformerBlock(
        (norm1): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (attn): Attention(
          (qkv): Conv2d(12, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (qkv_dwconv): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)
          (project_out): Conv2d(12, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        (norm2): LayerNorm(
          (body): WithBias_LayerNorm()
        )
        (ffn): FeedForward(
          (project_in): Conv2d(12, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (project_out): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
      )
    )
    (prompt): PromptGenBlock(
      (linear_layer): Linear(in_features=12, out_features=3, bias=True)
      (conv3x3): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (fuse): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
25-11-18 02:01:54.946 - INFO: Loading model for G [/mnt/ssd/BG/EditGuard/EditGuard/checkpoints/clean.pth] ...
25-11-18 02:01:55.095 - WARNING: Params [module.bitencoder.conv1.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.095 - WARNING: Params [module.bitencoder.conv1.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.conv1.layers.1.layers.0.weight] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.conv1.layers.1.layers.0.bias] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down1.layer.0.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down1.layer.0.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down1.layer.1.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down1.layer.1.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down1.layer.1.layers.1.layers.0.weight] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down1.layer.1.layers.1.layers.0.bias] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down2.layer.0.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down2.layer.0.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down2.layer.1.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down2.layer.1.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down2.layer.1.layers.1.layers.0.weight] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down2.layer.1.layers.1.layers.0.bias] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down3.layer.0.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down3.layer.0.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down3.layer.1.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down3.layer.1.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down3.layer.1.layers.1.layers.0.weight] will not optimize.
25-11-18 02:01:55.096 - WARNING: Params [module.bitencoder.down3.layer.1.layers.1.layers.0.bias] will not optimize.
25-11-18 02:01:55.097 - WARNING: Params [module.bitencoder.down4.layer.0.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.097 - WARNING: Params [module.bitencoder.down4.layer.0.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.097 - WARNING: Params [module.bitencoder.down4.layer.1.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.097 - WARNING: Params [module.bitencoder.down4.layer.1.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.097 - WARNING: Params [module.bitencoder.down4.layer.1.layers.1.layers.0.weight] will not optimize.
25-11-18 02:01:55.097 - WARNING: Params [module.bitencoder.down4.layer.1.layers.1.layers.0.bias] will not optimize.
25-11-18 02:01:55.097 - WARNING: Params [module.bitencoder.up3.conv.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.097 - WARNING: Params [module.bitencoder.up3.conv.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.097 - WARNING: Params [module.bitencoder.linear3.weight] will not optimize.
25-11-18 02:01:55.097 - WARNING: Params [module.bitencoder.linear3.bias] will not optimize.
25-11-18 02:01:55.097 - WARNING: Params [module.bitencoder.Conv_message3.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.097 - WARNING: Params [module.bitencoder.Conv_message3.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.097 - WARNING: Params [module.bitencoder.Conv_message3.layers.1.layers.0.weight] will not optimize.
25-11-18 02:01:55.097 - WARNING: Params [module.bitencoder.Conv_message3.layers.1.layers.0.bias] will not optimize.
25-11-18 02:01:55.097 - WARNING: Params [module.bitencoder.att3.layers.0.change.0.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att3.layers.0.left.0.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att3.layers.0.left.3.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att3.layers.0.left.6.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att3.layers.0.attention.se.1.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att3.layers.0.attention.se.3.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att3.layers.1.left.0.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att3.layers.1.left.3.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att3.layers.1.left.6.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att3.layers.1.attention.se.1.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att3.layers.1.attention.se.3.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.up2.conv.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.up2.conv.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.linear2.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.linear2.bias] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.Conv_message2.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.Conv_message2.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.Conv_message2.layers.1.layers.0.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.Conv_message2.layers.1.layers.0.bias] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att2.layers.0.change.0.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att2.layers.0.left.0.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att2.layers.0.left.3.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att2.layers.0.left.6.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att2.layers.0.attention.se.1.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att2.layers.0.attention.se.3.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att2.layers.1.left.0.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att2.layers.1.left.3.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att2.layers.1.left.6.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att2.layers.1.attention.se.1.weight] will not optimize.
25-11-18 02:01:55.098 - WARNING: Params [module.bitencoder.att2.layers.1.attention.se.3.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.up1.conv.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.up1.conv.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.linear1.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.linear1.bias] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.Conv_message1.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.Conv_message1.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.Conv_message1.layers.1.layers.0.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.Conv_message1.layers.1.layers.0.bias] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.att1.layers.0.change.0.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.att1.layers.0.left.0.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.att1.layers.0.left.3.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.att1.layers.0.left.6.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.att1.layers.0.attention.se.1.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.att1.layers.0.attention.se.3.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.att1.layers.1.left.0.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.att1.layers.1.left.3.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.att1.layers.1.left.6.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.att1.layers.1.attention.se.1.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.att1.layers.1.attention.se.3.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.up0.conv.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.up0.conv.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.linear0.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.linear0.bias] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.Conv_message0.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.Conv_message0.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.Conv_message0.layers.1.layers.0.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.Conv_message0.layers.1.layers.0.bias] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.att0.layers.0.change.0.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.att0.layers.0.left.0.weight] will not optimize.
25-11-18 02:01:55.099 - WARNING: Params [module.bitencoder.att0.layers.0.left.3.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitencoder.att0.layers.0.left.6.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitencoder.att0.layers.0.attention.se.1.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitencoder.att0.layers.0.attention.se.3.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitencoder.att0.layers.1.left.0.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitencoder.att0.layers.1.left.3.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitencoder.att0.layers.1.left.6.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitencoder.att0.layers.1.attention.se.1.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitencoder.att0.layers.1.attention.se.3.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitencoder.Conv_1x1.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitencoder.Conv_1x1.bias] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.conv1.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.conv1.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.conv1.layers.1.layers.0.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.conv1.layers.1.layers.0.bias] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.down1.layer.0.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.down1.layer.0.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.down1.layer.1.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.down1.layer.1.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.down1.layer.1.layers.1.layers.0.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.down1.layer.1.layers.1.layers.0.bias] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.down2.layer.0.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.down2.layer.0.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.down2.layer.1.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.down2.layer.1.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.down2.layer.1.layers.1.layers.0.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.down2.layer.1.layers.1.layers.0.bias] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.down3.layer.0.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.100 - WARNING: Params [module.bitdecoder.down3.layer.0.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.down3.layer.1.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.down3.layer.1.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.down3.layer.1.layers.1.layers.0.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.down3.layer.1.layers.1.layers.0.bias] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.down4.layer.0.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.down4.layer.0.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.down4.layer.1.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.down4.layer.1.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.down4.layer.1.layers.1.layers.0.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.down4.layer.1.layers.1.layers.0.bias] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.up3.conv.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.up3.conv.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.att3.layers.0.change.0.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.att3.layers.0.left.0.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.att3.layers.0.left.3.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.att3.layers.0.left.6.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.att3.layers.0.attention.se.1.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.att3.layers.0.attention.se.3.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.att3.layers.1.left.0.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.att3.layers.1.left.3.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.att3.layers.1.left.6.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.att3.layers.1.attention.se.1.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.att3.layers.1.attention.se.3.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.up2.conv.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.up2.conv.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.att2.layers.0.change.0.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.att2.layers.0.left.0.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.att2.layers.0.left.3.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.att2.layers.0.left.6.weight] will not optimize.
25-11-18 02:01:55.101 - WARNING: Params [module.bitdecoder.att2.layers.0.attention.se.1.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att2.layers.0.attention.se.3.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att2.layers.1.left.0.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att2.layers.1.left.3.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att2.layers.1.left.6.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att2.layers.1.attention.se.1.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att2.layers.1.attention.se.3.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.up1.conv.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.up1.conv.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att1.layers.0.change.0.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att1.layers.0.left.0.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att1.layers.0.left.3.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att1.layers.0.left.6.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att1.layers.0.attention.se.1.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att1.layers.0.attention.se.3.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att1.layers.1.left.0.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att1.layers.1.left.3.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att1.layers.1.left.6.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att1.layers.1.attention.se.1.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att1.layers.1.attention.se.3.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.up0.conv.layers.0.layers.0.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.up0.conv.layers.0.layers.0.bias] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att0.layers.0.change.0.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att0.layers.0.left.0.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att0.layers.0.left.3.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att0.layers.0.left.6.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att0.layers.0.attention.se.1.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att0.layers.0.attention.se.3.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att0.layers.1.left.0.weight] will not optimize.
25-11-18 02:01:55.102 - WARNING: Params [module.bitdecoder.att0.layers.1.left.3.weight] will not optimize.
25-11-18 02:01:55.103 - WARNING: Params [module.bitdecoder.att0.layers.1.left.6.weight] will not optimize.
25-11-18 02:01:55.103 - WARNING: Params [module.bitdecoder.att0.layers.1.attention.se.1.weight] will not optimize.
25-11-18 02:01:55.103 - WARNING: Params [module.bitdecoder.att0.layers.1.attention.se.3.weight] will not optimize.
25-11-18 02:01:55.103 - WARNING: Params [module.bitdecoder.Conv_1x1.weight] will not optimize.
25-11-18 02:01:55.103 - WARNING: Params [module.bitdecoder.message_layer.weight] will not optimize.
25-11-18 02:01:55.103 - WARNING: Params [module.bitdecoder.message_layer.bias] will not optimize.
25-11-18 02:01:55.104 - INFO: Model [Model_VSN] is created.
25-11-18 02:01:55.104 - INFO: Start training from epoch: 0, iter: 0
export CUDA_VISIBLE_DEVICES=0
Disabled distributed training.
Path already exists. Rename it to [/mnt/ssd/BG/EditGuard/EditGuard/experiments/editguard_coco_finetune_archived_251118-020153]
train
train
SEAttention
SEAttention
SEAttention
SEAttention
SEAttention
SEAttention
SEAttention
SEAttention
SEAttention
SEAttention
SEAttention
SEAttention
SEAttention
SEAttention
SEAttention
SEAttention
25-11-18 02:02:52.327 - INFO: <epoch:  0, iter:     100, lr:2.000e-04> l_center_x: 2.4839e+02 l_back_rec: 1.8327e+02 l_forw_fit: 2.9751e+02 l_msg: 0.0000e+00 l_h: 2.4839e+03 
25-11-18 02:03:42.217 - INFO: <epoch:  0, iter:     200, lr:2.000e-04> l_center_x: 2.9278e+01 l_back_rec: 8.2592e+01 l_forw_fit: 1.4355e+02 l_msg: 0.0000e+00 l_h: 2.9278e+02 
25-11-18 02:04:32.123 - INFO: <epoch:  0, iter:     300, lr:2.000e-04> l_center_x: 4.9254e+01 l_back_rec: 4.2287e+01 l_forw_fit: 7.3242e+01 l_msg: 0.0000e+00 l_h: 4.9254e+02 
25-11-18 02:05:22.143 - INFO: <epoch:  0, iter:     400, lr:2.000e-04> l_center_x: 1.9551e+01 l_back_rec: 4.8341e+01 l_forw_fit: 6.9119e+01 l_msg: 0.0000e+00 l_h: 1.9551e+02 
25-11-18 02:06:12.143 - INFO: <epoch:  0, iter:     500, lr:2.000e-04> l_center_x: 3.4745e+01 l_back_rec: 5.4296e+01 l_forw_fit: 6.0472e+01 l_msg: 0.0000e+00 l_h: 3.4745e+02 
25-11-18 02:07:02.161 - INFO: <epoch:  0, iter:     600, lr:2.000e-04> l_center_x: 2.8838e+01 l_back_rec: 7.2496e+01 l_forw_fit: 8.9059e+01 l_msg: 0.0000e+00 l_h: 2.8838e+02 
25-11-18 02:07:52.122 - INFO: <epoch:  0, iter:     700, lr:2.000e-04> l_center_x: 3.2992e+01 l_back_rec: 5.4374e+01 l_forw_fit: 9.1596e+01 l_msg: 0.0000e+00 l_h: 3.2992e+02 
25-11-18 02:08:42.143 - INFO: <epoch:  0, iter:     800, lr:2.000e-04> l_center_x: 9.6165e+00 l_back_rec: 1.4652e+02 l_forw_fit: 1.4144e+02 l_msg: 0.0000e+00 l_h: 9.6165e+01 
25-11-18 02:09:32.043 - INFO: <epoch:  0, iter:     900, lr:2.000e-04> l_center_x: 7.6624e+00 l_back_rec: 2.7526e+01 l_forw_fit: 4.2778e+01 l_msg: 0.0000e+00 l_h: 7.6624e+01 
25-11-18 02:10:22.344 - INFO: <epoch:  0, iter:   1,000, lr:2.000e-04> l_center_x: 1.1958e+01 l_back_rec: 3.0120e+02 l_forw_fit: 2.5812e+02 l_msg: 0.0000e+00 l_h: 1.1958e+02 
25-11-18 02:11:12.124 - INFO: <epoch:  0, iter:   1,100, lr:2.000e-04> l_center_x: 6.2064e+00 l_back_rec: 2.9205e+01 l_forw_fit: 2.8745e+01 l_msg: 0.0000e+00 l_h: 6.2064e+01 
25-11-18 02:12:02.124 - INFO: <epoch:  0, iter:   1,200, lr:2.000e-04> l_center_x: 4.9777e+00 l_back_rec: 1.4500e+01 l_forw_fit: 2.1439e+01 l_msg: 0.0000e+00 l_h: 4.9777e+01 
25-11-18 02:12:52.127 - INFO: <epoch:  0, iter:   1,300, lr:2.000e-04> l_center_x: 4.9227e+00 l_back_rec: 1.7141e+01 l_forw_fit: 2.9852e+01 l_msg: 0.0000e+00 l_h: 4.9227e+01 
25-11-18 02:13:42.117 - INFO: <epoch:  0, iter:   1,400, lr:2.000e-04> l_center_x: 1.5323e+01 l_back_rec: 6.3775e+01 l_forw_fit: 6.6756e+01 l_msg: 0.0000e+00 l_h: 1.5323e+02 
25-11-18 02:14:32.128 - INFO: <epoch:  0, iter:   1,500, lr:2.000e-04> l_center_x: 6.4292e+00 l_back_rec: 3.5852e+01 l_forw_fit: 3.3497e+01 l_msg: 0.0000e+00 l_h: 6.4292e+01 
25-11-18 02:15:21.826 - INFO: <epoch:  0, iter:   1,600, lr:2.000e-04> l_center_x: 4.4537e+00 l_back_rec: 1.5566e+01 l_forw_fit: 1.8699e+01 l_msg: 0.0000e+00 l_h: 4.4537e+01 
25-11-18 02:16:11.640 - INFO: <epoch:  0, iter:   1,700, lr:2.000e-04> l_center_x: 4.8620e+00 l_back_rec: 5.4352e+01 l_forw_fit: 6.2433e+01 l_msg: 0.0000e+00 l_h: 4.8620e+01 
25-11-18 02:17:01.544 - INFO: <epoch:  0, iter:   1,800, lr:2.000e-04> l_center_x: 1.2382e+01 l_back_rec: 4.8528e+01 l_forw_fit: 5.7324e+01 l_msg: 0.0000e+00 l_h: 1.2382e+02 
25-11-18 02:17:51.450 - INFO: <epoch:  0, iter:   1,900, lr:2.000e-04> l_center_x: 3.3616e+00 l_back_rec: 2.4288e+01 l_forw_fit: 2.1523e+01 l_msg: 0.0000e+00 l_h: 3.3616e+01 
25-11-18 02:18:41.342 - INFO: <epoch:  0, iter:   2,000, lr:2.000e-04> l_center_x: 4.6936e+00 l_back_rec: 5.1910e+01 l_forw_fit: 4.5867e+01 l_msg: 0.0000e+00 l_h: 4.6936e+01 
25-11-18 02:19:31.048 - INFO: <epoch:  0, iter:   2,100, lr:2.000e-04> l_center_x: 2.4765e+01 l_back_rec: 2.5553e+01 l_forw_fit: 2.2852e+01 l_msg: 0.0000e+00 l_h: 2.4765e+02 
25-11-18 02:20:21.117 - INFO: <epoch:  0, iter:   2,200, lr:2.000e-04> l_center_x: 8.6267e+00 l_back_rec: 1.0855e+02 l_forw_fit: 9.1200e+01 l_msg: 0.0000e+00 l_h: 8.6267e+01 
25-11-18 02:21:11.031 - INFO: <epoch:  0, iter:   2,300, lr:2.000e-04> l_center_x: 9.4919e+00 l_back_rec: 2.2156e+01 l_forw_fit: 4.2507e+01 l_msg: 0.0000e+00 l_h: 9.4919e+01 
25-11-18 02:22:01.129 - INFO: <epoch:  0, iter:   2,400, lr:2.000e-04> l_center_x: 1.0228e+01 l_back_rec: 7.7667e+01 l_forw_fit: 9.0477e+01 l_msg: 0.0000e+00 l_h: 1.0228e+02 
25-11-18 02:22:51.124 - INFO: <epoch:  0, iter:   2,500, lr:2.000e-04> l_center_x: 5.3248e+00 l_back_rec: 1.3649e+02 l_forw_fit: 1.3047e+02 l_msg: 0.0000e+00 l_h: 5.3248e+01 
25-11-18 02:22:51.124 - INFO: [SAVE] Begin saving models at iter 2500
25-11-18 02:22:51.301 - INFO: [SAVE] End saving models at iter 2500
25-11-18 02:23:41.122 - INFO: <epoch:  0, iter:   2,600, lr:2.000e-04> l_center_x: 5.2522e+00 l_back_rec: 7.6820e+01 l_forw_fit: 6.8115e+01 l_msg: 0.0000e+00 l_h: 5.2522e+01 
25-11-18 02:24:31.244 - INFO: <epoch:  0, iter:   2,700, lr:2.000e-04> l_center_x: 3.4623e+00 l_back_rec: 2.2792e+01 l_forw_fit: 2.3881e+01 l_msg: 0.0000e+00 l_h: 3.4623e+01 
25-11-18 02:25:20.825 - INFO: <epoch:  0, iter:   2,800, lr:2.000e-04> l_center_x: 2.7336e+00 l_back_rec: 1.2249e+02 l_forw_fit: 1.1853e+02 l_msg: 0.0000e+00 l_h: 2.7336e+01 
25-11-18 02:26:10.627 - INFO: <epoch:  0, iter:   2,900, lr:2.000e-04> l_center_x: 3.0653e+00 l_back_rec: 2.8331e+01 l_forw_fit: 6.6578e+01 l_msg: 0.0000e+00 l_h: 3.0653e+01 
25-11-18 02:27:00.640 - INFO: <epoch:  0, iter:   3,000, lr:2.000e-04> l_center_x: 8.0560e+00 l_back_rec: 2.9266e+01 l_forw_fit: 6.2301e+01 l_msg: 0.0000e+00 l_h: 8.0560e+01 
25-11-18 02:27:50.639 - INFO: <epoch:  0, iter:   3,100, lr:2.000e-04> l_center_x: 3.2118e+00 l_back_rec: 2.1502e+01 l_forw_fit: 5.4746e+01 l_msg: 0.0000e+00 l_h: 3.2118e+01 
25-11-18 02:28:40.651 - INFO: <epoch:  0, iter:   3,200, lr:2.000e-04> l_center_x: 5.0477e+00 l_back_rec: 1.6181e+01 l_forw_fit: 2.6212e+01 l_msg: 0.0000e+00 l_h: 5.0477e+01 
25-11-18 02:29:30.643 - INFO: <epoch:  0, iter:   3,300, lr:2.000e-04> l_center_x: 9.8638e+00 l_back_rec: 6.2165e+01 l_forw_fit: 5.0754e+01 l_msg: 0.0000e+00 l_h: 9.8638e+01 
25-11-18 02:30:20.630 - INFO: <epoch:  0, iter:   3,400, lr:2.000e-04> l_center_x: 4.6314e+00 l_back_rec: 4.7953e+01 l_forw_fit: 4.1374e+01 l_msg: 0.0000e+00 l_h: 4.6314e+01 
25-11-18 02:31:10.627 - INFO: <epoch:  0, iter:   3,500, lr:2.000e-04> l_center_x: 3.2845e+00 l_back_rec: 3.8022e+01 l_forw_fit: 3.5606e+01 l_msg: 0.0000e+00 l_h: 3.2845e+01 
25-11-18 02:32:00.629 - INFO: <epoch:  0, iter:   3,600, lr:2.000e-04> l_center_x: 7.1386e+00 l_back_rec: 2.1504e+01 l_forw_fit: 2.3770e+01 l_msg: 0.0000e+00 l_h: 7.1386e+01 
25-11-18 02:32:50.644 - INFO: <epoch:  0, iter:   3,700, lr:2.000e-04> l_center_x: 3.1167e+00 l_back_rec: 4.5780e+01 l_forw_fit: 4.6070e+01 l_msg: 0.0000e+00 l_h: 3.1167e+01 
25-11-18 02:33:40.645 - INFO: <epoch:  0, iter:   3,800, lr:2.000e-04> l_center_x: 3.9296e+00 l_back_rec: 1.8523e+01 l_forw_fit: 3.0888e+01 l_msg: 0.0000e+00 l_h: 3.9296e+01 
25-11-18 02:34:30.629 - INFO: <epoch:  0, iter:   3,900, lr:2.000e-04> l_center_x: 4.6504e+00 l_back_rec: 6.4170e+01 l_forw_fit: 4.9027e+01 l_msg: 0.0000e+00 l_h: 4.6504e+01 
25-11-18 02:35:20.629 - INFO: <epoch:  0, iter:   4,000, lr:2.000e-04> l_center_x: 8.9092e+00 l_back_rec: 4.6780e+01 l_forw_fit: 5.1425e+01 l_msg: 0.0000e+00 l_h: 8.9092e+01 
25-11-18 02:36:10.626 - INFO: <epoch:  0, iter:   4,100, lr:2.000e-04> l_center_x: 3.0666e+00 l_back_rec: 1.3669e+01 l_forw_fit: 2.2507e+01 l_msg: 0.0000e+00 l_h: 3.0666e+01 
25-11-18 02:37:00.625 - INFO: <epoch:  0, iter:   4,200, lr:2.000e-04> l_center_x: 2.0231e+00 l_back_rec: 1.6928e+01 l_forw_fit: 2.1980e+01 l_msg: 0.0000e+00 l_h: 2.0231e+01 
25-11-18 02:37:50.718 - INFO: <epoch:  0, iter:   4,300, lr:2.000e-04> l_center_x: 1.8332e+00 l_back_rec: 4.2425e+01 l_forw_fit: 4.0824e+01 l_msg: 0.0000e+00 l_h: 1.8332e+01 
25-11-18 02:38:40.343 - INFO: <epoch:  0, iter:   4,400, lr:2.000e-04> l_center_x: 6.4242e+00 l_back_rec: 2.7298e+01 l_forw_fit: 2.2901e+01 l_msg: 0.0000e+00 l_h: 6.4242e+01 
25-11-18 02:39:30.251 - INFO: <epoch:  0, iter:   4,500, lr:2.000e-04> l_center_x: 4.5432e+00 l_back_rec: 2.9488e+01 l_forw_fit: 5.9798e+01 l_msg: 0.0000e+00 l_h: 4.5432e+01 
25-11-18 02:40:20.138 - INFO: <epoch:  0, iter:   4,600, lr:2.000e-04> l_center_x: 5.0609e+00 l_back_rec: 5.0040e+01 l_forw_fit: 5.8881e+01 l_msg: 0.0000e+00 l_h: 5.0609e+01 
25-11-18 02:41:10.124 - INFO: <epoch:  0, iter:   4,700, lr:2.000e-04> l_center_x: 2.4390e+00 l_back_rec: 1.8824e+01 l_forw_fit: 2.3543e+01 l_msg: 0.0000e+00 l_h: 2.4390e+01 
25-11-18 02:42:00.128 - INFO: <epoch:  0, iter:   4,800, lr:2.000e-04> l_center_x: 4.9993e+00 l_back_rec: 4.7578e+01 l_forw_fit: 4.4379e+01 l_msg: 0.0000e+00 l_h: 4.9993e+01 
25-11-18 02:42:50.041 - INFO: <epoch:  0, iter:   4,900, lr:2.000e-04> l_center_x: 2.9812e+00 l_back_rec: 2.0997e+01 l_forw_fit: 2.9527e+01 l_msg: 1.1378e-02 l_h: 2.9812e+01 
25-11-18 02:43:40.141 - INFO: <epoch:  0, iter:   5,000, lr:2.000e-04> l_center_x: 4.8425e+00 l_back_rec: 1.6828e+02 l_forw_fit: 1.4121e+02 l_msg: 0.0000e+00 l_h: 4.8425e+01 
25-11-18 02:43:40.142 - INFO: [VAL] Begin validation at iter 5000
25-11-18 02:43:47.450 - INFO: [VAL] End validation at iter 5000
25-11-18 02:43:47.451 - INFO: # Validation # PSNR_Cover: 4.2290e+01, PSNR_Secret: _5.8995e+01, PSNR_Stego: 4.1937e+01, Bit_acc:  0.0000e+00
25-11-18 02:43:47.451 - INFO: <epoch:  0, iter:   5,000> PSNR_Cover: 4.2290e+01, PSNR_Secret: _5.8995e+01, PSNR_Stego: 4.1937e+01, Bit_acc:  0.0000e+00
25-11-18 02:43:47.451 - INFO: [SAVE] Begin saving models at iter 5000
25-11-18 02:43:47.615 - INFO: [SAVE] End saving models at iter 5000
25-11-18 02:44:37.549 - INFO: <epoch:  0, iter:   5,100, lr:2.000e-04> l_center_x: 2.5182e+00 l_back_rec: 2.7195e+01 l_forw_fit: 2.9431e+01 l_msg: 0.0000e+00 l_h: 2.5182e+01 
25-11-18 02:45:27.429 - INFO: <epoch:  0, iter:   5,200, lr:2.000e-04> l_center_x: 2.0616e+00 l_back_rec: 4.2686e+01 l_forw_fit: 3.8671e+01 l_msg: 0.0000e+00 l_h: 2.0616e+01 
25-11-18 02:46:17.531 - INFO: <epoch:  0, iter:   5,300, lr:2.000e-04> l_center_x: 1.9268e+00 l_back_rec: 6.2064e+01 l_forw_fit: 9.4391e+01 l_msg: 0.0000e+00 l_h: 1.9268e+01 
25-11-18 02:47:07.629 - INFO: <epoch:  0, iter:   5,400, lr:2.000e-04> l_center_x: 1.1378e+00 l_back_rec: 1.9028e+01 l_forw_fit: 2.6058e+01 l_msg: 0.0000e+00 l_h: 1.1378e+01 
25-11-18 02:47:57.630 - INFO: <epoch:  0, iter:   5,500, lr:2.000e-04> l_center_x: 7.5597e+00 l_back_rec: 7.4453e+01 l_forw_fit: 8.1438e+01 l_msg: 0.0000e+00 l_h: 7.5597e+01 
25-11-18 02:48:47.630 - INFO: <epoch:  0, iter:   5,600, lr:2.000e-04> l_center_x: 1.5021e+00 l_back_rec: 2.0887e+01 l_forw_fit: 2.3605e+01 l_msg: 0.0000e+00 l_h: 1.5021e+01 
25-11-18 02:49:37.641 - INFO: <epoch:  0, iter:   5,700, lr:2.000e-04> l_center_x: 4.3239e+00 l_back_rec: 6.4905e+01 l_forw_fit: 5.9757e+01 l_msg: 0.0000e+00 l_h: 4.3239e+01 
25-11-18 02:50:27.523 - INFO: <epoch:  0, iter:   5,800, lr:2.000e-04> l_center_x: 5.1993e+01 l_back_rec: 1.1251e+02 l_forw_fit: 1.1276e+02 l_msg: 0.0000e+00 l_h: 5.1993e+02 
25-11-18 02:51:17.530 - INFO: <epoch:  0, iter:   5,900, lr:2.000e-04> l_center_x: 2.5399e+01 l_back_rec: 7.3268e+01 l_forw_fit: 8.2746e+01 l_msg: 0.0000e+00 l_h: 2.5399e+02 
25-11-18 02:52:07.543 - INFO: <epoch:  0, iter:   6,000, lr:2.000e-04> l_center_x: 6.6104e+00 l_back_rec: 4.7401e+01 l_forw_fit: 3.4439e+01 l_msg: 2.7983e-03 l_h: 6.6104e+01 
25-11-18 02:52:57.523 - INFO: <epoch:  0, iter:   6,100, lr:2.000e-04> l_center_x: 1.2084e+01 l_back_rec: 5.0980e+01 l_forw_fit: 8.3021e+01 l_msg: 0.0000e+00 l_h: 1.2084e+02 
25-11-18 02:53:47.531 - INFO: <epoch:  0, iter:   6,200, lr:2.000e-04> l_center_x: 3.1138e+00 l_back_rec: 3.0303e+01 l_forw_fit: 2.5979e+01 l_msg: 1.1665e-02 l_h: 3.1138e+01 
25-11-18 02:54:37.328 - INFO: <epoch:  0, iter:   6,300, lr:2.000e-04> l_center_x: 9.0916e+00 l_back_rec: 1.4471e+02 l_forw_fit: 1.6306e+02 l_msg: 0.0000e+00 l_h: 9.0916e+01 
25-11-18 02:55:27.536 - INFO: <epoch:  0, iter:   6,400, lr:2.000e-04> l_center_x: 3.2714e+00 l_back_rec: 3.1778e+01 l_forw_fit: 3.4424e+01 l_msg: 0.0000e+00 l_h: 3.2714e+01 
25-11-18 02:56:17.424 - INFO: <epoch:  0, iter:   6,500, lr:2.000e-04> l_center_x: 2.7329e+00 l_back_rec: 2.4692e+01 l_forw_fit: 2.1508e+01 l_msg: 0.0000e+00 l_h: 2.7329e+01 
25-11-18 02:57:07.119 - INFO: <epoch:  0, iter:   6,600, lr:2.000e-04> l_center_x: 1.8542e+00 l_back_rec: 1.7576e+01 l_forw_fit: 2.1657e+01 l_msg: 1.8266e-05 l_h: 1.8542e+01 
25-11-18 02:57:57.123 - INFO: <epoch:  0, iter:   6,700, lr:2.000e-04> l_center_x: 2.4820e+00 l_back_rec: 1.6991e+01 l_forw_fit: 2.9423e+01 l_msg: 0.0000e+00 l_h: 2.4820e+01 
25-11-18 02:58:47.123 - INFO: <epoch:  0, iter:   6,800, lr:2.000e-04> l_center_x: 3.5569e+00 l_back_rec: 5.1153e+01 l_forw_fit: 6.7034e+01 l_msg: 0.0000e+00 l_h: 3.5569e+01 
25-11-18 02:59:37.144 - INFO: <epoch:  0, iter:   6,900, lr:2.000e-04> l_center_x: 2.1938e+00 l_back_rec: 1.0904e+01 l_forw_fit: 1.4642e+01 l_msg: 0.0000e+00 l_h: 2.1938e+01 
25-11-18 03:00:27.143 - INFO: <epoch:  0, iter:   7,000, lr:2.000e-04> l_center_x: 8.8267e+00 l_back_rec: 7.7127e+01 l_forw_fit: 6.4059e+01 l_msg: 0.0000e+00 l_h: 8.8267e+01 
25-11-18 03:01:17.329 - INFO: <epoch:  0, iter:   7,100, lr:2.000e-04> l_center_x: 2.4757e+00 l_back_rec: 2.2944e+01 l_forw_fit: 2.4605e+01 l_msg: 0.0000e+00 l_h: 2.4757e+01 
25-11-18 03:02:07.341 - INFO: <epoch:  0, iter:   7,200, lr:2.000e-04> l_center_x: 1.2973e+00 l_back_rec: 1.5493e+01 l_forw_fit: 1.4238e+01 l_msg: 0.0000e+00 l_h: 1.2973e+01 
25-11-18 03:02:57.129 - INFO: <epoch:  0, iter:   7,300, lr:2.000e-04> l_center_x: 7.1375e+00 l_back_rec: 1.2642e+02 l_forw_fit: 1.5375e+02 l_msg: 0.0000e+00 l_h: 7.1375e+01 
25-11-18 03:03:47.129 - INFO: <epoch:  0, iter:   7,400, lr:2.000e-04> l_center_x: 4.0305e+00 l_back_rec: 4.1405e+01 l_forw_fit: 5.8216e+01 l_msg: 0.0000e+00 l_h: 4.0305e+01 
25-11-18 03:04:37.041 - INFO: <epoch:  0, iter:   7,500, lr:2.000e-04> l_center_x: 2.6037e+00 l_back_rec: 3.5835e+01 l_forw_fit: 4.7488e+01 l_msg: 0.0000e+00 l_h: 2.6037e+01 
25-11-18 03:04:37.041 - INFO: [SAVE] Begin saving models at iter 7500
25-11-18 03:04:37.206 - INFO: [SAVE] End saving models at iter 7500
25-11-18 03:05:27.142 - INFO: <epoch:  0, iter:   7,600, lr:2.000e-04> l_center_x: 7.5744e+00 l_back_rec: 2.1041e+01 l_forw_fit: 3.6646e+01 l_msg: 0.0000e+00 l_h: 7.5744e+01 
25-11-18 03:06:17.145 - INFO: <epoch:  0, iter:   7,700, lr:2.000e-04> l_center_x: 3.5608e+01 l_back_rec: 3.8095e+02 l_forw_fit: 2.1672e+02 l_msg: 0.0000e+00 l_h: 3.5608e+02 
25-11-18 03:07:07.139 - INFO: <epoch:  0, iter:   7,800, lr:2.000e-04> l_center_x: 2.4385e+00 l_back_rec: 2.1877e+01 l_forw_fit: 2.7743e+01 l_msg: 0.0000e+00 l_h: 2.4385e+01 
25-11-18 03:07:57.220 - INFO: <epoch:  0, iter:   7,900, lr:2.000e-04> l_center_x: 3.1523e+00 l_back_rec: 2.8176e+01 l_forw_fit: 3.1237e+01 l_msg: 0.0000e+00 l_h: 3.1523e+01 
25-11-18 03:08:47.129 - INFO: <epoch:  0, iter:   8,000, lr:2.000e-04> l_center_x: 3.4999e+01 l_back_rec: 1.7262e+02 l_forw_fit: 2.2418e+02 l_msg: 1.4053e-03 l_h: 3.4999e+02 
25-11-18 03:09:36.831 - INFO: <epoch:  0, iter:   8,100, lr:2.000e-04> l_center_x: 3.6983e+04 l_back_rec: 1.5240e+05 l_forw_fit: 5.8959e+04 l_msg: 4.5332e-02 l_h: 3.6983e+05 
25-11-18 03:10:26.630 - INFO: <epoch:  0, iter:   8,200, lr:2.000e-04> l_center_x: 1.0327e+03 l_back_rec: 1.4152e+03 l_forw_fit: 1.5809e+03 l_msg: 2.7664e-03 l_h: 1.0327e+04 
25-11-18 03:11:16.631 - INFO: <epoch:  0, iter:   8,300, lr:2.000e-04> l_center_x: 3.6828e+02 l_back_rec: 8.0415e+02 l_forw_fit: 5.0646e+02 l_msg: 0.0000e+00 l_h: 3.6828e+03 
25-11-18 03:12:06.628 - INFO: <epoch:  0, iter:   8,400, lr:2.000e-04> l_center_x: 2.9599e+02 l_back_rec: 5.5419e+02 l_forw_fit: 3.3219e+02 l_msg: 0.0000e+00 l_h: 2.9599e+03 
25-11-18 03:12:56.624 - INFO: <epoch:  0, iter:   8,500, lr:2.000e-04> l_center_x: 1.7646e+02 l_back_rec: 3.6294e+02 l_forw_fit: 2.9306e+02 l_msg: 0.0000e+00 l_h: 1.7646e+03 
25-11-18 03:13:46.651 - INFO: <epoch:  0, iter:   8,600, lr:2.000e-04> l_center_x: 2.0554e+02 l_back_rec: 6.8626e+02 l_forw_fit: 5.3922e+02 l_msg: 4.1610e-05 l_h: 2.0554e+03 
25-11-18 03:14:36.729 - INFO: <epoch:  0, iter:   8,700, lr:2.000e-04> l_center_x: 1.6181e+02 l_back_rec: 2.5234e+02 l_forw_fit: 1.5773e+02 l_msg: 0.0000e+00 l_h: 1.6181e+03 
25-11-18 03:15:26.631 - INFO: <epoch:  0, iter:   8,800, lr:2.000e-04> l_center_x: 8.3671e+01 l_back_rec: 1.5488e+02 l_forw_fit: 1.4069e+02 l_msg: 0.0000e+00 l_h: 8.3671e+02 
25-11-18 03:16:16.629 - INFO: <epoch:  0, iter:   8,900, lr:2.000e-04> l_center_x: 2.2211e+02 l_back_rec: 4.8396e+02 l_forw_fit: 2.5576e+02 l_msg: 9.1892e-02 l_h: 2.2211e+03 
25-11-18 03:17:06.628 - INFO: <epoch:  0, iter:   9,000, lr:2.000e-04> l_center_x: 6.2718e+01 l_back_rec: 1.9070e+02 l_forw_fit: 1.2256e+02 l_msg: 0.0000e+00 l_h: 6.2718e+02 
25-11-18 03:17:56.626 - INFO: <epoch:  0, iter:   9,100, lr:2.000e-04> l_center_x: 1.1048e+02 l_back_rec: 1.7406e+02 l_forw_fit: 1.0404e+02 l_msg: 0.0000e+00 l_h: 1.1048e+03 
25-11-18 03:18:46.631 - INFO: <epoch:  0, iter:   9,200, lr:2.000e-04> l_center_x: 6.1229e+01 l_back_rec: 1.6415e+02 l_forw_fit: 1.3009e+02 l_msg: 0.0000e+00 l_h: 6.1229e+02 
25-11-18 03:19:36.630 - INFO: <epoch:  0, iter:   9,300, lr:2.000e-04> l_center_x: 1.2381e+02 l_back_rec: 3.0037e+02 l_forw_fit: 2.0559e+02 l_msg: 0.0000e+00 l_h: 1.2381e+03 
25-11-18 03:20:26.629 - INFO: <epoch:  0, iter:   9,400, lr:2.000e-04> l_center_x: 3.9092e+01 l_back_rec: 1.1797e+02 l_forw_fit: 1.1038e+02 l_msg: 0.0000e+00 l_h: 3.9092e+02 
25-11-18 03:21:16.642 - INFO: <epoch:  0, iter:   9,500, lr:2.000e-04> l_center_x: 3.9484e+01 l_back_rec: 1.0905e+02 l_forw_fit: 8.0482e+01 l_msg: 0.0000e+00 l_h: 3.9484e+02 
25-11-18 03:22:06.623 - INFO: <epoch:  0, iter:   9,600, lr:2.000e-04> l_center_x: 3.7178e+01 l_back_rec: 6.4628e+01 l_forw_fit: 5.9672e+01 l_msg: 0.0000e+00 l_h: 3.7178e+02 
25-11-18 03:22:56.629 - INFO: <epoch:  0, iter:   9,700, lr:2.000e-04> l_center_x: 5.8041e+01 l_back_rec: 9.6903e+01 l_forw_fit: 6.9906e+01 l_msg: 0.0000e+00 l_h: 5.8041e+02 
25-11-18 03:23:46.629 - INFO: <epoch:  0, iter:   9,800, lr:2.000e-04> l_center_x: 3.9583e+01 l_back_rec: 1.2124e+02 l_forw_fit: 7.9447e+01 l_msg: 0.0000e+00 l_h: 3.9583e+02 
25-11-18 03:24:36.627 - INFO: <epoch:  0, iter:   9,900, lr:2.000e-04> l_center_x: 6.7878e+01 l_back_rec: 9.3549e+01 l_forw_fit: 7.8676e+01 l_msg: 0.0000e+00 l_h: 6.7878e+02 
25-11-18 03:25:26.643 - INFO: <epoch:  0, iter:  10,000, lr:2.000e-04> l_center_x: 3.0954e+01 l_back_rec: 7.7996e+01 l_forw_fit: 5.1602e+01 l_msg: 0.0000e+00 l_h: 3.0954e+02 
25-11-18 03:25:26.644 - INFO: [VAL] Begin validation at iter 10000
25-11-18 03:25:33.927 - INFO: [VAL] End validation at iter 10000
25-11-18 03:25:33.928 - INFO: # Validation # PSNR_Cover: 3.8980e+01, PSNR_Secret: _4.7368e+01, PSNR_Stego: 3.9490e+01, Bit_acc:  0.0000e+00
25-11-18 03:25:33.928 - INFO: <epoch:  0, iter:  10,000> PSNR_Cover: 3.8980e+01, PSNR_Secret: _4.7368e+01, PSNR_Stego: 3.9490e+01, Bit_acc:  0.0000e+00
25-11-18 03:25:33.928 - INFO: [SAVE] Begin saving models at iter 10000
25-11-18 03:25:34.097 - INFO: [SAVE] End saving models at iter 10000
25-11-18 03:26:24.030 - INFO: <epoch:  0, iter:  10,100, lr:2.000e-04> l_center_x: 5.3627e+01 l_back_rec: 7.6686e+01 l_forw_fit: 6.1569e+01 l_msg: 0.0000e+00 l_h: 5.3627e+02 
25-11-18 03:27:14.159 - INFO: <epoch:  0, iter:  10,200, lr:2.000e-04> l_center_x: 4.3589e+01 l_back_rec: 1.0037e+02 l_forw_fit: 8.9135e+01 l_msg: 0.0000e+00 l_h: 4.3589e+02 
25-11-18 03:28:04.131 - INFO: <epoch:  0, iter:  10,300, lr:2.000e-04> l_center_x: 6.4810e+01 l_back_rec: 1.0179e+02 l_forw_fit: 6.4492e+01 l_msg: 4.4102e-04 l_h: 6.4810e+02 
25-11-18 03:28:54.131 - INFO: <epoch:  0, iter:  10,400, lr:2.000e-04> l_center_x: 3.1755e+01 l_back_rec: 2.5905e+02 l_forw_fit: 2.7300e+02 l_msg: 0.0000e+00 l_h: 3.1755e+02 
25-11-18 03:29:44.174 - INFO: <epoch:  0, iter:  10,500, lr:2.000e-04> l_center_x: 4.5782e+01 l_back_rec: 9.6159e+01 l_forw_fit: 5.2811e+01 l_msg: 0.0000e+00 l_h: 4.5782e+02 
25-11-18 03:30:34.141 - INFO: <epoch:  0, iter:  10,600, lr:2.000e-04> l_center_x: 6.7070e+01 l_back_rec: 8.7031e+02 l_forw_fit: 7.5201e+02 l_msg: 0.0000e+00 l_h: 6.7070e+02 
25-11-18 03:31:24.130 - INFO: <epoch:  0, iter:  10,700, lr:2.000e-04> l_center_x: 7.5164e+01 l_back_rec: 6.2793e+01 l_forw_fit: 5.8271e+01 l_msg: 0.0000e+00 l_h: 7.5164e+02 
25-11-18 03:32:14.141 - INFO: <epoch:  0, iter:  10,800, lr:2.000e-04> l_center_x: 2.7015e+01 l_back_rec: 1.1387e+02 l_forw_fit: 9.8720e+01 l_msg: 0.0000e+00 l_h: 2.7015e+02 
25-11-18 03:33:04.129 - INFO: <epoch:  0, iter:  10,900, lr:2.000e-04> l_center_x: 2.1114e+01 l_back_rec: 6.0406e+01 l_forw_fit: 6.2790e+01 l_msg: 0.0000e+00 l_h: 2.1114e+02 
25-11-18 03:33:54.130 - INFO: <epoch:  0, iter:  11,000, lr:2.000e-04> l_center_x: 1.0043e+02 l_back_rec: 2.0475e+02 l_forw_fit: 2.7257e+02 l_msg: 0.0000e+00 l_h: 1.0043e+03 
25-11-18 03:34:44.130 - INFO: <epoch:  0, iter:  11,100, lr:2.000e-04> l_center_x: 2.6407e+01 l_back_rec: 7.9326e+01 l_forw_fit: 6.2434e+01 l_msg: 2.8830e-06 l_h: 2.6407e+02 
25-11-18 03:35:34.328 - INFO: <epoch:  0, iter:  11,200, lr:2.000e-04> l_center_x: 3.1637e+01 l_back_rec: 9.3892e+01 l_forw_fit: 1.2698e+02 l_msg: 4.8146e-03 l_h: 3.1637e+02 
25-11-18 03:36:24.316 - INFO: <epoch:  0, iter:  11,300, lr:2.000e-04> l_center_x: 1.5205e+01 l_back_rec: 1.4457e+02 l_forw_fit: 1.2421e+02 l_msg: 0.0000e+00 l_h: 1.5205e+02 
25-11-18 03:37:14.116 - INFO: <epoch:  0, iter:  11,400, lr:2.000e-04> l_center_x: 1.4362e+01 l_back_rec: 1.0081e+02 l_forw_fit: 8.0504e+01 l_msg: 0.0000e+00 l_h: 1.4362e+02 
25-11-18 03:38:04.144 - INFO: <epoch:  0, iter:  11,500, lr:2.000e-04> l_center_x: 2.0065e+01 l_back_rec: 5.3905e+01 l_forw_fit: 1.0117e+02 l_msg: 0.0000e+00 l_h: 2.0065e+02 
25-11-18 03:38:54.143 - INFO: <epoch:  0, iter:  11,600, lr:2.000e-04> l_center_x: 2.4836e+01 l_back_rec: 5.1574e+01 l_forw_fit: 3.7217e+01 l_msg: 0.0000e+00 l_h: 2.4836e+02 
25-11-18 03:39:44.129 - INFO: <epoch:  0, iter:  11,700, lr:2.000e-04> l_center_x: 2.4053e+01 l_back_rec: 1.1418e+02 l_forw_fit: 6.8085e+01 l_msg: 0.0000e+00 l_h: 2.4053e+02 
25-11-18 03:40:34.117 - INFO: <epoch:  0, iter:  11,800, lr:2.000e-04> l_center_x: 2.5683e+01 l_back_rec: 6.0070e+01 l_forw_fit: 4.2115e+01 l_msg: 3.6036e-04 l_h: 2.5683e+02 
25-11-18 03:41:24.128 - INFO: <epoch:  0, iter:  11,900, lr:2.000e-04> l_center_x: 4.4725e+01 l_back_rec: 6.1030e+01 l_forw_fit: 6.8105e+01 l_msg: 0.0000e+00 l_h: 4.4725e+02 
25-11-18 03:42:14.024 - INFO: <epoch:  0, iter:  12,000, lr:2.000e-04> l_center_x: 2.8899e+01 l_back_rec: 1.0947e+02 l_forw_fit: 1.0223e+02 l_msg: 0.0000e+00 l_h: 2.8899e+02 
25-11-18 03:43:04.030 - INFO: <epoch:  0, iter:  12,100, lr:2.000e-04> l_center_x: 1.2656e+01 l_back_rec: 5.5097e+01 l_forw_fit: 4.8099e+01 l_msg: 0.0000e+00 l_h: 1.2656e+02 
25-11-18 03:43:53.916 - INFO: <epoch:  0, iter:  12,200, lr:2.000e-04> l_center_x: 1.4757e+01 l_back_rec: 1.1585e+02 l_forw_fit: 7.8083e+01 l_msg: 0.0000e+00 l_h: 1.4757e+02 
25-11-18 03:44:43.826 - INFO: <epoch:  0, iter:  12,300, lr:2.000e-04> l_center_x: 1.4297e+01 l_back_rec: 5.1770e+01 l_forw_fit: 5.5934e+01 l_msg: 0.0000e+00 l_h: 1.4297e+02 
25-11-18 03:45:33.629 - INFO: <epoch:  0, iter:  12,400, lr:2.000e-04> l_center_x: 1.4776e+01 l_back_rec: 7.0930e+01 l_forw_fit: 6.7137e+01 l_msg: 0.0000e+00 l_h: 1.4776e+02 
25-11-18 03:46:23.640 - INFO: <epoch:  0, iter:  12,500, lr:2.000e-04> l_center_x: 3.0296e+01 l_back_rec: 5.5467e+01 l_forw_fit: 6.4122e+01 l_msg: 0.0000e+00 l_h: 3.0296e+02 
25-11-18 03:46:23.641 - INFO: [SAVE] Begin saving models at iter 12500
25-11-18 03:46:23.804 - INFO: [SAVE] End saving models at iter 12500
25-11-18 03:47:13.851 - INFO: <epoch:  0, iter:  12,600, lr:2.000e-04> l_center_x: 7.2268e+00 l_back_rec: 5.4042e+01 l_forw_fit: 4.8331e+01 l_msg: 0.0000e+00 l_h: 7.2268e+01 
25-11-18 03:48:03.644 - INFO: <epoch:  0, iter:  12,700, lr:2.000e-04> l_center_x: 1.5452e+01 l_back_rec: 8.1438e+01 l_forw_fit: 5.1327e+01 l_msg: 0.0000e+00 l_h: 1.5452e+02 
25-11-18 03:48:53.629 - INFO: <epoch:  0, iter:  12,800, lr:2.000e-04> l_center_x: 1.1396e+01 l_back_rec: 6.4581e+01 l_forw_fit: 5.4335e+01 l_msg: 0.0000e+00 l_h: 1.1396e+02 
25-11-18 03:49:43.540 - INFO: <epoch:  0, iter:  12,900, lr:2.000e-04> l_center_x: 4.4334e+01 l_back_rec: 2.3134e+02 l_forw_fit: 2.6590e+02 l_msg: 0.0000e+00 l_h: 4.4334e+02 
25-11-18 03:50:33.529 - INFO: <epoch:  0, iter:  13,000, lr:2.000e-04> l_center_x: 1.0329e+01 l_back_rec: 3.2095e+01 l_forw_fit: 3.1384e+01 l_msg: 0.0000e+00 l_h: 1.0329e+02 
25-11-18 03:51:23.629 - INFO: <epoch:  0, iter:  13,100, lr:2.000e-04> l_center_x: 1.1712e+01 l_back_rec: 6.3465e+01 l_forw_fit: 4.8656e+01 l_msg: 0.0000e+00 l_h: 1.1712e+02 
25-11-18 03:52:13.429 - INFO: <epoch:  0, iter:  13,200, lr:2.000e-04> l_center_x: 1.5249e+01 l_back_rec: 2.4708e+02 l_forw_fit: 2.5916e+02 l_msg: 0.0000e+00 l_h: 1.5249e+02 
25-11-18 03:53:03.240 - INFO: <epoch:  0, iter:  13,300, lr:2.000e-04> l_center_x: 1.1119e+01 l_back_rec: 2.9613e+01 l_forw_fit: 2.4644e+01 l_msg: 2.2992e-03 l_h: 1.1119e+02 
25-11-18 03:53:53.130 - INFO: <epoch:  0, iter:  13,400, lr:2.000e-04> l_center_x: 3.0325e+01 l_back_rec: 4.6025e+01 l_forw_fit: 2.6235e+01 l_msg: 0.0000e+00 l_h: 3.0325e+02 
25-11-18 03:54:43.130 - INFO: <epoch:  0, iter:  13,500, lr:2.000e-04> l_center_x: 2.3178e+01 l_back_rec: 1.6981e+02 l_forw_fit: 1.6158e+02 l_msg: 0.0000e+00 l_h: 2.3178e+02 
25-11-18 03:55:33.126 - INFO: <epoch:  0, iter:  13,600, lr:2.000e-04> l_center_x: 1.6182e+01 l_back_rec: 7.2098e+01 l_forw_fit: 8.6058e+01 l_msg: 0.0000e+00 l_h: 1.6182e+02 
25-11-18 03:56:23.361 - INFO: <epoch:  0, iter:  13,700, lr:2.000e-04> l_center_x: 1.6451e+01 l_back_rec: 6.1610e+01 l_forw_fit: 4.9062e+01 l_msg: 0.0000e+00 l_h: 1.6451e+02 
25-11-18 03:57:13.352 - INFO: <epoch:  0, iter:  13,800, lr:2.000e-04> l_center_x: 1.4381e+01 l_back_rec: 3.6954e+01 l_forw_fit: 2.8148e+01 l_msg: 0.0000e+00 l_h: 1.4381e+02 
25-11-18 03:58:03.324 - INFO: <epoch:  0, iter:  13,900, lr:2.000e-04> l_center_x: 9.5146e+00 l_back_rec: 1.1174e+02 l_forw_fit: 1.3636e+02 l_msg: 0.0000e+00 l_h: 9.5146e+01 
25-11-18 03:58:53.429 - INFO: <epoch:  0, iter:  14,000, lr:2.000e-04> l_center_x: 2.5916e+01 l_back_rec: 5.9379e+01 l_forw_fit: 5.9235e+01 l_msg: 9.3994e-04 l_h: 2.5916e+02 
25-11-18 03:59:43.637 - INFO: <epoch:  0, iter:  14,100, lr:2.000e-04> l_center_x: 3.4902e+01 l_back_rec: 1.4731e+02 l_forw_fit: 7.2573e+01 l_msg: 0.0000e+00 l_h: 3.4902e+02 
25-11-18 04:00:33.853 - INFO: <epoch:  0, iter:  14,200, lr:2.000e-04> l_center_x: 1.0569e+01 l_back_rec: 3.0016e+01 l_forw_fit: 3.1925e+01 l_msg: 0.0000e+00 l_h: 1.0569e+02 
25-11-18 04:01:23.943 - INFO: <epoch:  0, iter:  14,300, lr:2.000e-04> l_center_x: 5.8680e+00 l_back_rec: 7.3798e+01 l_forw_fit: 9.3932e+01 l_msg: 0.0000e+00 l_h: 5.8680e+01 
25-11-18 04:02:14.142 - INFO: <epoch:  0, iter:  14,400, lr:2.000e-04> l_center_x: 1.8313e+01 l_back_rec: 2.0939e+02 l_forw_fit: 1.9253e+02 l_msg: 0.0000e+00 l_h: 1.8313e+02 
25-11-18 04:03:04.127 - INFO: <epoch:  0, iter:  14,500, lr:2.000e-04> l_center_x: 8.0341e+00 l_back_rec: 4.2079e+01 l_forw_fit: 4.1397e+01 l_msg: 0.0000e+00 l_h: 8.0341e+01 
25-11-18 04:03:54.151 - INFO: <epoch:  0, iter:  14,600, lr:2.000e-04> l_center_x: 9.0897e+00 l_back_rec: 4.5882e+01 l_forw_fit: 4.6168e+01 l_msg: 0.0000e+00 l_h: 9.0897e+01 
25-11-18 04:04:44.149 - INFO: <epoch:  0, iter:  14,700, lr:2.000e-04> l_center_x: 6.9704e+00 l_back_rec: 3.8599e+01 l_forw_fit: 3.9965e+01 l_msg: 0.0000e+00 l_h: 6.9704e+01 
25-11-18 04:05:34.230 - INFO: <epoch:  0, iter:  14,800, lr:2.000e-04> l_center_x: 2.2176e+05 l_back_rec: 5.9922e+05 l_forw_fit: 2.2308e+05 l_msg: 2.3285e-01 l_h: 2.2176e+06 
25-11-18 04:06:24.317 - INFO: <epoch:  0, iter:  14,900, lr:2.000e-04> l_center_x: 9.9723e+06 l_back_rec: 1.0023e+07 l_forw_fit: 2.1234e+05 l_msg: 3.5369e-01 l_h: 9.9723e+07 
25-11-18 04:07:14.179 - INFO: <epoch:  0, iter:  15,000, lr:2.000e-04> l_center_x: 6.9177e+05 l_back_rec: 4.5357e+05 l_forw_fit: 2.2053e+05 l_msg: 3.6662e-01 l_h: 6.9177e+06 
25-11-18 04:07:14.179 - INFO: [VAL] Begin validation at iter 15000
25-11-18 04:07:22.446 - INFO: [VAL] End validation at iter 15000
25-11-18 04:07:22.446 - INFO: # Validation # PSNR_Cover: 6.5982e+00, PSNR_Secret: _4.6928e+00, PSNR_Stego: 5.5930e+00, Bit_acc:  5.0469e-01
25-11-18 04:07:22.446 - INFO: <epoch:  0, iter:  15,000> PSNR_Cover: 6.5982e+00, PSNR_Secret: _4.6928e+00, PSNR_Stego: 5.5930e+00, Bit_acc:  5.0469e-01
25-11-18 04:07:22.446 - INFO: [SAVE] Begin saving models at iter 15000
25-11-18 04:07:22.612 - INFO: [SAVE] End saving models at iter 15000
25-11-18 04:08:12.643 - INFO: <epoch:  0, iter:  15,100, lr:2.000e-04> l_center_x: 5.6185e+05 l_back_rec: 4.7423e+05 l_forw_fit: 3.1688e+05 l_msg: 4.7541e-01 l_h: 5.6185e+06 
25-11-18 04:09:02.629 - INFO: <epoch:  0, iter:  15,200, lr:2.000e-04> l_center_x: 4.5751e+05 l_back_rec: 3.2931e+05 l_forw_fit: 2.3739e+05 l_msg: 3.2790e-01 l_h: 4.5751e+06 
25-11-18 04:09:52.630 - INFO: <epoch:  0, iter:  15,300, lr:2.000e-04> l_center_x: 4.0818e+05 l_back_rec: 3.0239e+05 l_forw_fit: 2.3201e+05 l_msg: 3.8198e-01 l_h: 4.0818e+06 
25-11-18 04:10:42.738 - INFO: <epoch:  0, iter:  15,400, lr:2.000e-04> l_center_x: 3.2809e+05 l_back_rec: 3.2539e+05 l_forw_fit: 2.4720e+05 l_msg: 3.6319e-01 l_h: 3.2809e+06 
25-11-18 04:11:32.628 - INFO: <epoch:  0, iter:  15,500, lr:2.000e-04> l_center_x: 3.0436e+05 l_back_rec: 3.0511e+05 l_forw_fit: 2.1170e+05 l_msg: 3.4455e-01 l_h: 3.0436e+06 
25-11-18 04:12:22.629 - INFO: <epoch:  0, iter:  15,600, lr:2.000e-04> l_center_x: 3.2808e+05 l_back_rec: 2.7831e+05 l_forw_fit: 2.2966e+05 l_msg: 3.8367e-01 l_h: 3.2808e+06 
25-11-18 04:13:12.629 - INFO: <epoch:  0, iter:  15,700, lr:2.000e-04> l_center_x: 2.8681e+05 l_back_rec: 2.6820e+05 l_forw_fit: 2.2482e+05 l_msg: 3.2483e-01 l_h: 2.8681e+06 
25-11-18 04:14:02.530 - INFO: <epoch:  0, iter:  15,800, lr:2.000e-04> l_center_x: 2.6991e+05 l_back_rec: 2.7787e+05 l_forw_fit: 2.2133e+05 l_msg: 3.2538e-01 l_h: 2.6991e+06 
25-11-18 04:14:52.551 - INFO: <epoch:  0, iter:  15,900, lr:2.000e-04> l_center_x: 2.8534e+05 l_back_rec: 1.8567e+05 l_forw_fit: 2.0353e+05 l_msg: 3.7046e-01 l_h: 2.8534e+06 
25-11-18 04:15:42.530 - INFO: <epoch:  0, iter:  16,000, lr:2.000e-04> l_center_x: 2.8443e+05 l_back_rec: 2.1371e+05 l_forw_fit: 2.2453e+05 l_msg: 2.9291e-01 l_h: 2.8443e+06 
25-11-18 04:16:32.619 - INFO: <epoch:  0, iter:  16,100, lr:2.000e-04> l_center_x: 2.5340e+05 l_back_rec: 2.7099e+05 l_forw_fit: 2.3917e+05 l_msg: 3.8034e-01 l_h: 2.5340e+06 
25-11-18 04:17:22.669 - INFO: <epoch:  0, iter:  16,200, lr:2.000e-04> l_center_x: 2.5519e+05 l_back_rec: 2.7181e+05 l_forw_fit: 2.5827e+05 l_msg: 3.0149e-01 l_h: 2.5519e+06 
25-11-18 04:18:12.643 - INFO: <epoch:  0, iter:  16,300, lr:2.000e-04> l_center_x: 2.4857e+05 l_back_rec: 1.8197e+05 l_forw_fit: 2.0778e+05 l_msg: 3.7383e-01 l_h: 2.4857e+06 
25-11-18 04:19:02.629 - INFO: <epoch:  0, iter:  16,400, lr:2.000e-04> l_center_x: 2.4984e+05 l_back_rec: 1.7746e+05 l_forw_fit: 2.2134e+05 l_msg: 2.8205e-01 l_h: 2.4984e+06 
25-11-18 04:19:52.629 - INFO: <epoch:  0, iter:  16,500, lr:2.000e-04> l_center_x: 2.2008e+05 l_back_rec: 2.2578e+05 l_forw_fit: 2.3052e+05 l_msg: 4.1487e-01 l_h: 2.2008e+06 
25-11-18 04:20:42.621 - INFO: <epoch:  0, iter:  16,600, lr:2.000e-04> l_center_x: 2.3178e+05 l_back_rec: 2.5467e+05 l_forw_fit: 2.2939e+05 l_msg: 3.8561e-01 l_h: 2.3178e+06 
25-11-18 04:21:32.651 - INFO: <epoch:  0, iter:  16,700, lr:2.000e-04> l_center_x: 2.1084e+05 l_back_rec: 1.9945e+05 l_forw_fit: 2.1982e+05 l_msg: 3.8366e-01 l_h: 2.1084e+06 
25-11-18 04:22:22.640 - INFO: <epoch:  0, iter:  16,800, lr:2.000e-04> l_center_x: 2.3141e+05 l_back_rec: 1.7754e+05 l_forw_fit: 2.2556e+05 l_msg: 3.8004e-01 l_h: 2.3141e+06 
25-11-18 04:23:12.648 - INFO: <epoch:  0, iter:  16,900, lr:2.000e-04> l_center_x: 2.0099e+05 l_back_rec: 2.5433e+05 l_forw_fit: 2.9308e+05 l_msg: 3.8269e-01 l_h: 2.0099e+06 
25-11-18 04:24:02.635 - INFO: <epoch:  0, iter:  17,000, lr:2.000e-04> l_center_x: 1.8894e+05 l_back_rec: 1.5725e+05 l_forw_fit: 1.9502e+05 l_msg: 3.4673e-01 l_h: 1.8894e+06 
25-11-18 04:24:52.629 - INFO: <epoch:  0, iter:  17,100, lr:2.000e-04> l_center_x: 2.0121e+05 l_back_rec: 2.2135e+05 l_forw_fit: 2.7237e+05 l_msg: 3.5091e-01 l_h: 2.0121e+06 
25-11-18 04:25:42.729 - INFO: <epoch:  0, iter:  17,200, lr:2.000e-04> l_center_x: 2.0282e+05 l_back_rec: 1.2234e+05 l_forw_fit: 2.2665e+05 l_msg: 3.1865e-01 l_h: 2.0282e+06 
25-11-18 04:26:32.644 - INFO: <epoch:  0, iter:  17,300, lr:2.000e-04> l_center_x: 1.8331e+05 l_back_rec: 1.4755e+05 l_forw_fit: 2.4351e+05 l_msg: 3.3937e-01 l_h: 1.8331e+06 
25-11-18 04:27:22.543 - INFO: <epoch:  0, iter:  17,400, lr:2.000e-04> l_center_x: 1.8411e+05 l_back_rec: 1.0388e+05 l_forw_fit: 2.4843e+05 l_msg: 3.2126e-01 l_h: 1.8411e+06 
25-11-18 04:28:22.408 - INFO: <epoch:  0, iter:  17,500, lr:2.000e-04> l_center_x: 1.7651e+05 l_back_rec: 1.1883e+05 l_forw_fit: 2.3663e+05 l_msg: 3.6729e-01 l_h: 1.7651e+06 
25-11-18 04:28:22.409 - INFO: [SAVE] Begin saving models at iter 17500
25-11-18 04:28:22.572 - INFO: [SAVE] End saving models at iter 17500
25-11-18 04:29:13.343 - INFO: <epoch:  0, iter:  17,600, lr:2.000e-04> l_center_x: 1.6851e+05 l_back_rec: 1.4395e+05 l_forw_fit: 2.5372e+05 l_msg: 3.8223e-01 l_h: 1.6851e+06 
25-11-18 04:30:03.637 - INFO: <epoch:  0, iter:  17,700, lr:2.000e-04> l_center_x: 1.6538e+05 l_back_rec: 1.3711e+05 l_forw_fit: 2.4732e+05 l_msg: 4.0035e-01 l_h: 1.6538e+06 
25-11-18 04:30:53.826 - INFO: <epoch:  0, iter:  17,800, lr:2.000e-04> l_center_x: 1.6233e+05 l_back_rec: 9.3785e+04 l_forw_fit: 2.4679e+05 l_msg: 4.3316e-01 l_h: 1.6233e+06 
25-11-18 04:31:43.842 - INFO: <epoch:  0, iter:  17,900, lr:2.000e-04> l_center_x: 1.8222e+05 l_back_rec: 1.7641e+05 l_forw_fit: 2.3443e+05 l_msg: 3.8133e-01 l_h: 1.8222e+06 
25-11-18 04:32:33.630 - INFO: <epoch:  0, iter:  18,000, lr:2.000e-04> l_center_x: 1.4822e+05 l_back_rec: 1.4100e+05 l_forw_fit: 2.3692e+05 l_msg: 3.6461e-01 l_h: 1.4822e+06 
25-11-18 04:33:23.629 - INFO: <epoch:  0, iter:  18,100, lr:2.000e-04> l_center_x: 1.4248e+05 l_back_rec: 3.5041e+05 l_forw_fit: 3.7239e+05 l_msg: 3.5538e-01 l_h: 1.4248e+06 
25-11-18 04:34:13.823 - INFO: <epoch:  0, iter:  18,200, lr:2.000e-04> l_center_x: 6.2463e+06 l_back_rec: 2.1144e+06 l_forw_fit: 2.5479e+05 l_msg: 3.7786e-01 l_h: 6.2463e+07 
25-11-18 04:35:03.629 - INFO: <epoch:  0, iter:  18,300, lr:2.000e-04> l_center_x: 6.2030e+05 l_back_rec: 4.3393e+05 l_forw_fit: 2.8871e+05 l_msg: 2.6757e-01 l_h: 6.2030e+06 
25-11-18 04:35:53.631 - INFO: <epoch:  0, iter:  18,400, lr:2.000e-04> l_center_x: 3.8596e+05 l_back_rec: 1.6216e+05 l_forw_fit: 2.0726e+05 l_msg: 3.4124e-01 l_h: 3.8596e+06 
25-11-18 04:36:43.629 - INFO: <epoch:  0, iter:  18,500, lr:2.000e-04> l_center_x: 3.1179e+05 l_back_rec: 1.1886e+05 l_forw_fit: 1.9060e+05 l_msg: 4.3662e-01 l_h: 3.1179e+06 
25-11-18 04:37:33.645 - INFO: <epoch:  0, iter:  18,600, lr:2.000e-04> l_center_x: 2.4269e+05 l_back_rec: 1.7390e+05 l_forw_fit: 2.4016e+05 l_msg: 3.9553e-01 l_h: 2.4269e+06 
25-11-18 04:38:23.629 - INFO: <epoch:  0, iter:  18,700, lr:2.000e-04> l_center_x: 2.1753e+05 l_back_rec: 2.0037e+05 l_forw_fit: 2.6932e+05 l_msg: 3.5066e-01 l_h: 2.1753e+06 
25-11-18 04:39:13.728 - INFO: <epoch:  0, iter:  18,800, lr:2.000e-04> l_center_x: 2.1669e+05 l_back_rec: 2.4284e+05 l_forw_fit: 2.4111e+05 l_msg: 4.4162e-01 l_h: 2.1669e+06 
25-11-18 04:40:03.630 - INFO: <epoch:  0, iter:  18,900, lr:2.000e-04> l_center_x: 2.4103e+05 l_back_rec: 3.1272e+05 l_forw_fit: 2.9293e+05 l_msg: 3.6925e-01 l_h: 2.4103e+06 
25-11-18 04:40:53.630 - INFO: <epoch:  0, iter:  19,000, lr:2.000e-04> l_center_x: 1.9569e+05 l_back_rec: 1.8810e+05 l_forw_fit: 2.2075e+05 l_msg: 5.4142e-01 l_h: 1.9569e+06 
25-11-18 04:41:43.630 - INFO: <epoch:  0, iter:  19,100, lr:2.000e-04> l_center_x: 2.1993e+05 l_back_rec: 3.0007e+05 l_forw_fit: 3.1291e+05 l_msg: 3.2655e-01 l_h: 2.1993e+06 
25-11-18 04:42:33.626 - INFO: <epoch:  0, iter:  19,200, lr:2.000e-04> l_center_x: 1.6698e+05 l_back_rec: 2.2793e+05 l_forw_fit: 2.1905e+05 l_msg: 5.3237e-01 l_h: 1.6698e+06 
25-11-18 04:43:23.625 - INFO: <epoch:  0, iter:  19,300, lr:2.000e-04> l_center_x: 1.8187e+05 l_back_rec: 2.1940e+05 l_forw_fit: 2.5071e+05 l_msg: 4.4638e-01 l_h: 1.8187e+06 
25-11-18 04:44:13.624 - INFO: <epoch:  0, iter:  19,400, lr:2.000e-04> l_center_x: 1.8440e+05 l_back_rec: 1.3690e+05 l_forw_fit: 2.2202e+05 l_msg: 3.6495e-01 l_h: 1.8440e+06 
25-11-18 04:45:03.661 - INFO: <epoch:  0, iter:  19,500, lr:2.000e-04> l_center_x: 1.7431e+05 l_back_rec: 1.5945e+05 l_forw_fit: 2.4368e+05 l_msg: 3.7247e-01 l_h: 1.7431e+06 
25-11-18 04:45:53.630 - INFO: <epoch:  0, iter:  19,600, lr:2.000e-04> l_center_x: 1.7502e+06 l_back_rec: 9.8064e+05 l_forw_fit: 2.9898e+05 l_msg: 3.3621e-01 l_h: 1.7502e+07 
25-11-18 04:46:44.729 - INFO: <epoch:  0, iter:  19,700, lr:2.000e-04> l_center_x: 4.5460e+05 l_back_rec: 2.1418e+05 l_forw_fit: 2.2138e+05 l_msg: 3.2845e-01 l_h: 4.5460e+06 
25-11-18 04:47:46.819 - INFO: <epoch:  0, iter:  19,800, lr:2.000e-04> l_center_x: 3.6316e+05 l_back_rec: 1.9872e+05 l_forw_fit: 2.4006e+05 l_msg: 4.1911e-01 l_h: 3.6316e+06 
25-11-18 04:48:37.926 - INFO: <epoch:  0, iter:  19,900, lr:2.000e-04> l_center_x: 6.9512e+05 l_back_rec: 4.8557e+05 l_forw_fit: 2.2396e+05 l_msg: 3.2188e-01 l_h: 6.9512e+06 
25-11-18 04:49:28.029 - INFO: <epoch:  0, iter:  20,000, lr:2.000e-04> l_center_x: 3.2844e+05 l_back_rec: 2.4884e+05 l_forw_fit: 2.5640e+05 l_msg: 4.2066e-01 l_h: 3.2844e+06 
25-11-18 04:49:28.030 - INFO: [VAL] Begin validation at iter 20000
25-11-18 04:49:35.891 - INFO: [VAL] End validation at iter 20000
25-11-18 04:49:35.892 - INFO: # Validation # PSNR_Cover: 8.3805e+00, PSNR_Secret: _6.6493e+00, PSNR_Stego: 5.2598e+00, Bit_acc:  5.0938e-01
25-11-18 04:49:35.892 - INFO: <epoch:  0, iter:  20,000> PSNR_Cover: 8.3805e+00, PSNR_Secret: _6.6493e+00, PSNR_Stego: 5.2598e+00, Bit_acc:  5.0938e-01
25-11-18 04:49:35.892 - INFO: [SAVE] Begin saving models at iter 20000
25-11-18 04:49:36.065 - INFO: [SAVE] End saving models at iter 20000
25-11-18 04:50:26.028 - INFO: <epoch:  0, iter:  20,100, lr:2.000e-04> l_center_x: 2.7102e+05 l_back_rec: 1.7508e+05 l_forw_fit: 2.1330e+05 l_msg: 3.9162e-01 l_h: 2.7102e+06 
25-11-18 04:51:16.140 - INFO: <epoch:  0, iter:  20,200, lr:2.000e-04> l_center_x: 2.8796e+05 l_back_rec: 2.1930e+05 l_forw_fit: 2.9612e+05 l_msg: 3.6083e-01 l_h: 2.8796e+06 
25-11-18 04:52:06.129 - INFO: <epoch:  0, iter:  20,300, lr:2.000e-04> l_center_x: 2.2384e+05 l_back_rec: 3.2607e+05 l_forw_fit: 3.1256e+05 l_msg: 3.3525e-01 l_h: 2.2384e+06 
25-11-18 04:52:56.129 - INFO: <epoch:  0, iter:  20,400, lr:2.000e-04> l_center_x: 2.3537e+05 l_back_rec: 1.4009e+05 l_forw_fit: 3.0160e+05 l_msg: 4.2827e-01 l_h: 2.3537e+06 
25-11-18 04:53:46.128 - INFO: <epoch:  0, iter:  20,500, lr:2.000e-04> l_center_x: 2.5108e+05 l_back_rec: 1.4658e+05 l_forw_fit: 2.2440e+05 l_msg: 3.2780e-02 l_h: 2.5108e+06 
25-11-18 04:54:36.136 - INFO: <epoch:  0, iter:  20,600, lr:2.000e-04> l_center_x: 2.1507e+05 l_back_rec: 1.6585e+05 l_forw_fit: 2.4653e+05 l_msg: 3.5882e-01 l_h: 2.1507e+06 
25-11-18 04:55:26.122 - INFO: <epoch:  0, iter:  20,700, lr:2.000e-04> l_center_x: 2.4678e+05 l_back_rec: 1.7785e+05 l_forw_fit: 2.5562e+05 l_msg: 2.4547e-01 l_h: 2.4678e+06 
25-11-18 04:56:16.127 - INFO: <epoch:  0, iter:  20,800, lr:2.000e-04> l_center_x: 2.3349e+05 l_back_rec: 1.8291e+05 l_forw_fit: 2.3602e+05 l_msg: 3.1614e-01 l_h: 2.3349e+06 
25-11-18 04:57:06.148 - INFO: <epoch:  0, iter:  20,900, lr:2.000e-04> l_center_x: 2.5294e+05 l_back_rec: 2.4543e+05 l_forw_fit: 2.8230e+05 l_msg: 3.6145e-01 l_h: 2.5294e+06 
25-11-18 04:57:56.126 - INFO: <epoch:  0, iter:  21,000, lr:2.000e-04> l_center_x: 2.0217e+05 l_back_rec: 1.1195e+05 l_forw_fit: 2.2517e+05 l_msg: 8.3772e-02 l_h: 2.0217e+06 
25-11-18 04:58:46.246 - INFO: <epoch:  0, iter:  21,100, lr:2.000e-04> l_center_x: 2.2858e+05 l_back_rec: 3.5300e+05 l_forw_fit: 2.9929e+05 l_msg: 2.6371e-01 l_h: 2.2858e+06 
25-11-18 04:59:36.156 - INFO: <epoch:  0, iter:  21,200, lr:2.000e-04> l_center_x: 2.5274e+05 l_back_rec: 1.5994e+05 l_forw_fit: 2.4188e+05 l_msg: 3.2592e-01 l_h: 2.5274e+06 
25-11-18 05:00:26.126 - INFO: <epoch:  0, iter:  21,300, lr:2.000e-04> l_center_x: 1.9761e+05 l_back_rec: 1.5316e+05 l_forw_fit: 2.2444e+05 l_msg: 2.7970e-01 l_h: 1.9761e+06 
25-11-18 05:01:16.144 - INFO: <epoch:  0, iter:  21,400, lr:2.000e-04> l_center_x: 2.1887e+05 l_back_rec: 2.3134e+05 l_forw_fit: 2.5855e+05 l_msg: 2.9821e-01 l_h: 2.1887e+06 
25-11-18 05:02:06.143 - INFO: <epoch:  0, iter:  21,500, lr:2.000e-04> l_center_x: 1.8483e+05 l_back_rec: 1.7598e+05 l_forw_fit: 2.3627e+05 l_msg: 3.6749e-01 l_h: 1.8483e+06 
25-11-18 05:02:56.223 - INFO: <epoch:  0, iter:  21,600, lr:2.000e-04> l_center_x: 8.7282e+05 l_back_rec: 3.6992e+05 l_forw_fit: 2.8863e+05 l_msg: 2.7706e-01 l_h: 8.7282e+06 
25-11-18 05:03:46.137 - INFO: <epoch:  0, iter:  21,700, lr:2.000e-04> l_center_x: 5.2782e+05 l_back_rec: 1.0546e+05 l_forw_fit: 1.9191e+05 l_msg: 2.1147e-01 l_h: 5.2782e+06 
25-11-18 05:04:36.240 - INFO: <epoch:  0, iter:  21,800, lr:2.000e-04> l_center_x: 3.3013e+05 l_back_rec: 1.4278e+05 l_forw_fit: 2.2299e+05 l_msg: 2.8890e-01 l_h: 3.3013e+06 
25-11-18 05:05:26.244 - INFO: <epoch:  0, iter:  21,900, lr:2.000e-04> l_center_x: 3.5496e+05 l_back_rec: 1.7595e+05 l_forw_fit: 2.8514e+05 l_msg: 2.5416e-01 l_h: 3.5496e+06 
25-11-18 05:06:16.128 - INFO: <epoch:  0, iter:  22,000, lr:2.000e-04> l_center_x: 2.7967e+05 l_back_rec: 7.4220e+04 l_forw_fit: 2.3737e+05 l_msg: 1.9670e-01 l_h: 2.7967e+06 
25-11-18 05:07:06.154 - INFO: <epoch:  0, iter:  22,100, lr:2.000e-04> l_center_x: 2.2516e+05 l_back_rec: 8.5600e+04 l_forw_fit: 2.3788e+05 l_msg: 2.8826e-01 l_h: 2.2516e+06 
25-11-18 05:07:56.232 - INFO: <epoch:  0, iter:  22,200, lr:2.000e-04> l_center_x: 2.9846e+05 l_back_rec: 1.5110e+05 l_forw_fit: 2.6625e+05 l_msg: 2.3753e-01 l_h: 2.9846e+06 
25-11-18 05:08:46.327 - INFO: <epoch:  0, iter:  22,300, lr:2.000e-04> l_center_x: 2.6800e+05 l_back_rec: 1.2231e+05 l_forw_fit: 2.0787e+05 l_msg: 1.3573e-01 l_h: 2.6800e+06 
25-11-18 05:09:36.428 - INFO: <epoch:  0, iter:  22,400, lr:2.000e-04> l_center_x: 2.2729e+05 l_back_rec: 8.3546e+04 l_forw_fit: 2.3061e+05 l_msg: 2.1481e-01 l_h: 2.2729e+06 
25-11-18 05:10:26.629 - INFO: <epoch:  0, iter:  22,500, lr:2.000e-04> l_center_x: 2.0205e+05 l_back_rec: 7.2416e+04 l_forw_fit: 2.3041e+05 l_msg: 2.7443e-01 l_h: 2.0205e+06 
25-11-18 05:10:26.629 - INFO: [SAVE] Begin saving models at iter 22500
25-11-18 05:10:26.795 - INFO: [SAVE] End saving models at iter 22500
25-11-18 05:11:16.843 - INFO: <epoch:  0, iter:  22,600, lr:2.000e-04> l_center_x: 2.4259e+05 l_back_rec: 1.1048e+05 l_forw_fit: 1.9143e+05 l_msg: 1.8436e-01 l_h: 2.4259e+06 
25-11-18 05:12:06.629 - INFO: <epoch:  0, iter:  22,700, lr:2.000e-04> l_center_x: 2.1120e+05 l_back_rec: 8.6519e+04 l_forw_fit: 2.5039e+05 l_msg: 3.4648e-01 l_h: 2.1120e+06 
25-11-18 05:12:56.630 - INFO: <epoch:  0, iter:  22,800, lr:2.000e-04> l_center_x: 2.2370e+05 l_back_rec: 8.5772e+04 l_forw_fit: 2.1383e+05 l_msg: 2.4789e-01 l_h: 2.2370e+06 
25-11-18 05:13:46.637 - INFO: <epoch:  0, iter:  22,900, lr:2.000e-04> l_center_x: 2.0496e+05 l_back_rec: 6.0960e+04 l_forw_fit: 2.0963e+05 l_msg: 3.3081e-01 l_h: 2.0496e+06 
25-11-18 05:14:36.624 - INFO: <epoch:  0, iter:  23,000, lr:2.000e-04> l_center_x: 1.8993e+05 l_back_rec: 1.1197e+05 l_forw_fit: 2.3335e+05 l_msg: 2.6374e-01 l_h: 1.8993e+06 
25-11-18 05:15:26.626 - INFO: <epoch:  0, iter:  23,100, lr:2.000e-04> l_center_x: 2.1770e+05 l_back_rec: 1.2199e+05 l_forw_fit: 1.7261e+05 l_msg: 1.9238e-01 l_h: 2.1770e+06 
25-11-18 05:16:16.656 - INFO: <epoch:  0, iter:  23,200, lr:2.000e-04> l_center_x: 1.6138e+05 l_back_rec: 4.3418e+04 l_forw_fit: 2.2257e+05 l_msg: 4.1953e-01 l_h: 1.6138e+06 
25-11-18 05:17:06.640 - INFO: <epoch:  0, iter:  23,300, lr:2.000e-04> l_center_x: 2.2966e+05 l_back_rec: 7.0763e+04 l_forw_fit: 1.7402e+05 l_msg: 3.5380e-01 l_h: 2.2966e+06 
25-11-18 05:17:56.629 - INFO: <epoch:  0, iter:  23,400, lr:2.000e-04> l_center_x: 1.9642e+05 l_back_rec: 9.4868e+04 l_forw_fit: 2.4085e+05 l_msg: 5.9090e-01 l_h: 1.9642e+06 
25-11-18 05:18:46.631 - INFO: <epoch:  0, iter:  23,500, lr:2.000e-04> l_center_x: 1.9119e+05 l_back_rec: 7.6763e+04 l_forw_fit: 1.8487e+05 l_msg: 2.2559e-01 l_h: 1.9119e+06 
25-11-18 05:19:36.625 - INFO: <epoch:  0, iter:  23,600, lr:2.000e-04> l_center_x: 1.8323e+05 l_back_rec: 7.3846e+04 l_forw_fit: 1.7321e+05 l_msg: 3.5318e-01 l_h: 1.8323e+06 
25-11-18 05:20:26.632 - INFO: <epoch:  0, iter:  23,700, lr:2.000e-04> l_center_x: 2.3026e+05 l_back_rec: 1.1787e+05 l_forw_fit: 2.0934e+05 l_msg: 2.7150e-01 l_h: 2.3026e+06 
25-11-18 05:21:16.631 - INFO: <epoch:  0, iter:  23,800, lr:2.000e-04> l_center_x: 1.7945e+05 l_back_rec: 9.5638e+04 l_forw_fit: 1.6846e+05 l_msg: 2.0165e-01 l_h: 1.7945e+06 
25-11-18 05:22:06.626 - INFO: <epoch:  0, iter:  23,900, lr:2.000e-04> l_center_x: 1.6789e+05 l_back_rec: 1.6024e+05 l_forw_fit: 1.6031e+05 l_msg: 1.3671e-01 l_h: 1.6789e+06 
25-11-18 05:23:05.727 - INFO: <epoch:  0, iter:  24,000, lr:2.000e-04> l_center_x: 1.3202e+05 l_back_rec: 1.0457e+05 l_forw_fit: 1.9009e+05 l_msg: 1.9716e-01 l_h: 1.3202e+06 
25-11-18 05:23:55.638 - INFO: <epoch:  0, iter:  24,100, lr:2.000e-04> l_center_x: 1.2268e+05 l_back_rec: 5.7386e+04 l_forw_fit: 1.1619e+05 l_msg: 3.7414e-01 l_h: 1.2268e+06 
25-11-18 05:24:45.530 - INFO: <epoch:  0, iter:  24,200, lr:2.000e-04> l_center_x: 1.5376e+05 l_back_rec: 1.2644e+05 l_forw_fit: 1.5247e+05 l_msg: 1.1675e-01 l_h: 1.5376e+06 
25-11-18 05:25:38.343 - INFO: <epoch:  0, iter:  24,300, lr:2.000e-04> l_center_x: 1.3256e+05 l_back_rec: 1.9688e+05 l_forw_fit: 3.4524e+05 l_msg: 4.7367e-01 l_h: 1.3256e+06 
25-11-18 05:26:28.239 - INFO: <epoch:  0, iter:  24,400, lr:2.000e-04> l_center_x: 1.0314e+05 l_back_rec: 7.8958e+04 l_forw_fit: 1.5532e+05 l_msg: 2.4044e-01 l_h: 1.0314e+06 
25-11-18 05:27:18.121 - INFO: <epoch:  0, iter:  24,500, lr:2.000e-04> l_center_x: 1.1875e+05 l_back_rec: 1.2572e+05 l_forw_fit: 1.2687e+05 l_msg: 2.9886e-02 l_h: 1.1875e+06 
25-11-18 05:28:08.330 - INFO: <epoch:  0, iter:  24,600, lr:2.000e-04> l_center_x: 1.1795e+05 l_back_rec: 7.8176e+04 l_forw_fit: 1.2804e+05 l_msg: 1.2460e-01 l_h: 1.1795e+06 
25-11-18 05:28:59.124 - INFO: <epoch:  0, iter:  24,700, lr:2.000e-04> l_center_x: 1.2831e+05 l_back_rec: 7.6648e+04 l_forw_fit: 1.0843e+05 l_msg: 2.0555e-01 l_h: 1.2831e+06 
25-11-18 05:29:49.140 - INFO: <epoch:  0, iter:  24,800, lr:2.000e-04> l_center_x: 9.6043e+04 l_back_rec: 7.2818e+04 l_forw_fit: 1.4893e+05 l_msg: 1.9789e-01 l_h: 9.6043e+05 
25-11-18 05:30:39.726 - INFO: <epoch:  0, iter:  24,900, lr:2.000e-04> l_center_x: 1.0633e+05 l_back_rec: 9.0849e+04 l_forw_fit: 1.9918e+05 l_msg: 2.0040e-01 l_h: 1.0633e+06 
25-11-18 05:31:29.629 - INFO: <epoch:  0, iter:  25,000, lr:2.000e-04> l_center_x: 9.0095e+04 l_back_rec: 5.4361e+04 l_forw_fit: 1.4448e+05 l_msg: 1.6076e-01 l_h: 9.0095e+05 
25-11-18 05:31:29.630 - INFO: [VAL] Begin validation at iter 25000
25-11-18 05:31:37.552 - INFO: [VAL] End validation at iter 25000
25-11-18 05:31:37.552 - INFO: # Validation # PSNR_Cover: 1.1396e+01, PSNR_Secret: _1.0204e+01, PSNR_Stego: 7.3377e+00, Bit_acc:  2.8594e-01
25-11-18 05:31:37.552 - INFO: <epoch:  0, iter:  25,000> PSNR_Cover: 1.1396e+01, PSNR_Secret: _1.0204e+01, PSNR_Stego: 7.3377e+00, Bit_acc:  2.8594e-01
25-11-18 05:31:37.553 - INFO: [SAVE] Begin saving models at iter 25000
25-11-18 05:31:37.731 - INFO: [SAVE] End saving models at iter 25000
25-11-18 05:32:27.749 - INFO: <epoch:  0, iter:  25,100, lr:2.000e-04> l_center_x: 1.0512e+05 l_back_rec: 7.4406e+04 l_forw_fit: 2.1463e+05 l_msg: 4.1267e-01 l_h: 1.0512e+06 
25-11-18 05:33:17.710 - INFO: <epoch:  0, iter:  25,200, lr:2.000e-04> l_center_x: 8.2494e+04 l_back_rec: 6.0515e+04 l_forw_fit: 1.1448e+05 l_msg: 2.6122e-01 l_h: 8.2494e+05 
25-11-18 05:34:08.339 - INFO: <epoch:  0, iter:  25,300, lr:2.000e-04> l_center_x: 9.1169e+04 l_back_rec: 5.8798e+04 l_forw_fit: 1.2139e+05 l_msg: 2.0315e-01 l_h: 9.1169e+05 
25-11-18 05:34:58.140 - INFO: <epoch:  0, iter:  25,400, lr:2.000e-04> l_center_x: 1.0547e+05 l_back_rec: 1.0661e+05 l_forw_fit: 2.1085e+05 l_msg: 1.4072e-01 l_h: 1.0547e+06 
25-11-18 05:35:48.347 - INFO: <epoch:  0, iter:  25,500, lr:2.000e-04> l_center_x: 7.4858e+04 l_back_rec: 5.0203e+04 l_forw_fit: 2.1849e+05 l_msg: 7.6935e-04 l_h: 7.4858e+05 
25-11-18 05:36:38.654 - INFO: <epoch:  0, iter:  25,600, lr:2.000e-04> l_center_x: 7.9893e+04 l_back_rec: 5.0851e+04 l_forw_fit: 1.1713e+05 l_msg: 1.1676e-01 l_h: 7.9893e+05 
25-11-18 05:37:29.311 - INFO: <epoch:  0, iter:  25,700, lr:2.000e-04> l_center_x: 7.3782e+04 l_back_rec: 4.3700e+04 l_forw_fit: 1.1864e+05 l_msg: 1.2625e-01 l_h: 7.3782e+05 
25-11-18 05:38:33.445 - INFO: <epoch:  0, iter:  25,800, lr:2.000e-04> l_center_x: 8.5060e+04 l_back_rec: 4.5727e+04 l_forw_fit: 1.0135e+05 l_msg: 8.1745e-02 l_h: 8.5060e+05 
25-11-18 05:39:33.148 - INFO: <epoch:  0, iter:  25,900, lr:2.000e-04> l_center_x: 9.1901e+04 l_back_rec: 7.7452e+04 l_forw_fit: 1.2822e+05 l_msg: 1.9913e-01 l_h: 9.1901e+05 
25-11-18 05:40:29.143 - INFO: <epoch:  0, iter:  26,000, lr:2.000e-04> l_center_x: 7.3825e+04 l_back_rec: 2.8826e+04 l_forw_fit: 1.1872e+05 l_msg: 1.9044e-02 l_h: 7.3825e+05 
25-11-18 05:41:36.248 - INFO: <epoch:  0, iter:  26,100, lr:2.000e-04> l_center_x: 6.6679e+04 l_back_rec: 3.3760e+04 l_forw_fit: 9.9053e+04 l_msg: 1.5403e-01 l_h: 6.6679e+05 
25-11-18 05:42:43.657 - INFO: <epoch:  0, iter:  26,200, lr:2.000e-04> l_center_x: 8.0249e+04 l_back_rec: 8.3529e+04 l_forw_fit: 1.6948e+05 l_msg: 3.3234e-01 l_h: 8.0249e+05 
25-11-18 05:43:51.459 - INFO: <epoch:  0, iter:  26,300, lr:2.000e-04> l_center_x: 7.7606e+04 l_back_rec: 4.9872e+04 l_forw_fit: 1.2406e+05 l_msg: 2.0160e-01 l_h: 7.7606e+05 
25-11-18 05:44:58.862 - INFO: <epoch:  0, iter:  26,400, lr:2.000e-04> l_center_x: 6.7400e+04 l_back_rec: 3.7783e+04 l_forw_fit: 1.0741e+05 l_msg: 9.6891e-02 l_h: 6.7400e+05 
25-11-18 05:46:05.643 - INFO: <epoch:  0, iter:  26,500, lr:2.000e-04> l_center_x: 7.4748e+04 l_back_rec: 2.9528e+04 l_forw_fit: 1.1548e+05 l_msg: 5.1027e-02 l_h: 7.4748e+05 
25-11-18 05:47:14.029 - INFO: <epoch:  0, iter:  26,600, lr:2.000e-04> l_center_x: 6.9113e+04 l_back_rec: 2.6924e+04 l_forw_fit: 7.1058e+04 l_msg: 1.5631e-01 l_h: 6.9113e+05 
25-11-18 05:48:21.697 - INFO: <epoch:  0, iter:  26,700, lr:2.000e-04> l_center_x: 6.2876e+04 l_back_rec: 3.9759e+04 l_forw_fit: 1.2985e+05 l_msg: 5.7995e-02 l_h: 6.2876e+05 
25-11-18 05:49:29.362 - INFO: <epoch:  0, iter:  26,800, lr:2.000e-04> l_center_x: 6.1174e+04 l_back_rec: 5.8731e+04 l_forw_fit: 1.1752e+05 l_msg: 1.0841e-01 l_h: 6.1174e+05 
25-11-18 05:50:36.640 - INFO: <epoch:  0, iter:  26,900, lr:2.000e-04> l_center_x: 6.3549e+04 l_back_rec: 4.9250e+04 l_forw_fit: 1.4942e+05 l_msg: 9.9995e-02 l_h: 6.3549e+05 
25-11-18 05:51:45.574 - INFO: <epoch:  0, iter:  27,000, lr:2.000e-04> l_center_x: 7.1468e+04 l_back_rec: 4.8913e+04 l_forw_fit: 1.2529e+05 l_msg: 2.3558e-01 l_h: 7.1468e+05 
25-11-18 05:52:53.864 - INFO: <epoch:  0, iter:  27,100, lr:2.000e-04> l_center_x: 5.9468e+04 l_back_rec: 2.8557e+04 l_forw_fit: 8.8756e+04 l_msg: 6.1816e-02 l_h: 5.9468e+05 
25-11-18 05:54:01.662 - INFO: <epoch:  0, iter:  27,200, lr:2.000e-04> l_center_x: 5.8474e+04 l_back_rec: 3.4287e+04 l_forw_fit: 1.2901e+05 l_msg: 5.0638e-02 l_h: 5.8474e+05 
25-11-18 05:55:09.535 - INFO: <epoch:  0, iter:  27,300, lr:2.000e-04> l_center_x: 5.6343e+04 l_back_rec: 2.7312e+04 l_forw_fit: 8.3210e+04 l_msg: 1.3313e-01 l_h: 5.6343e+05 
25-11-18 05:56:16.910 - INFO: <epoch:  0, iter:  27,400, lr:2.000e-04> l_center_x: 5.7380e+04 l_back_rec: 3.5478e+04 l_forw_fit: 1.4980e+05 l_msg: 1.8844e-01 l_h: 5.7380e+05 
25-11-18 05:57:23.148 - INFO: <epoch:  0, iter:  27,500, lr:2.000e-04> l_center_x: 5.9430e+04 l_back_rec: 3.5494e+04 l_forw_fit: 1.0284e+05 l_msg: 1.5947e-01 l_h: 5.9430e+05 
25-11-18 05:57:23.149 - INFO: [SAVE] Begin saving models at iter 27500
25-11-18 05:57:23.320 - INFO: [SAVE] End saving models at iter 27500
25-11-18 05:58:31.875 - INFO: <epoch:  0, iter:  27,600, lr:2.000e-04> l_center_x: 5.1447e+04 l_back_rec: 1.8490e+04 l_forw_fit: 4.9931e+04 l_msg: 3.1277e-02 l_h: 5.1447e+05 
25-11-18 05:59:24.340 - INFO: <epoch:  0, iter:  27,700, lr:2.000e-04> l_center_x: 9.1075e+04 l_back_rec: 5.2249e+04 l_forw_fit: 1.0077e+05 l_msg: 5.5193e-02 l_h: 9.1075e+05 
25-11-18 06:00:14.319 - INFO: <epoch:  0, iter:  27,800, lr:2.000e-04> l_center_x: 5.5124e+04 l_back_rec: 5.8116e+04 l_forw_fit: 8.7408e+04 l_msg: 2.0251e-01 l_h: 5.5124e+05 
25-11-18 06:01:04.298 - INFO: <epoch:  0, iter:  27,900, lr:2.000e-04> l_center_x: 5.5547e+04 l_back_rec: 2.8667e+04 l_forw_fit: 7.4853e+04 l_msg: 1.0286e-01 l_h: 5.5547e+05 
25-11-18 06:01:54.129 - INFO: <epoch:  0, iter:  28,000, lr:2.000e-04> l_center_x: 4.9395e+04 l_back_rec: 2.7684e+04 l_forw_fit: 4.2094e+04 l_msg: 3.3454e-02 l_h: 4.9395e+05 
25-11-18 06:02:44.130 - INFO: <epoch:  0, iter:  28,100, lr:2.000e-04> l_center_x: 5.8113e+04 l_back_rec: 3.8664e+04 l_forw_fit: 5.5947e+04 l_msg: 2.3858e-01 l_h: 5.8113e+05 
